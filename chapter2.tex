%===================================== CHAP 3 =================================

\chapter{Theory}
In this chapter, blablabla...
\section{Sensor Fusion}
\label{section:fusion}
Robots and autonomous vehicles, be it a car or a ship, operate in unstructured environments, environments that are unpredictable. While robots working on a assembly line work in very structured, predictable environments, a vessel attempting to atonomously navigate a busy shipping lane has to rely on sensor inputs in order to make sense of its environment, and navigate safely. In sensing its environment, a vessel might use various sensors, such as radar(s), cameras, sonars or laser range finders. The different sensors typically give a incomplete and imperfect view of the surrounding world. Sensor data fusion is the process of combining such mutually complementary sensor information in such a way that a better understanding of the surroundings can be achieved \cite{sensorFusion1}.
The U.S. Joint Directors of Laboratories (JDL) Data Fusion Working Group developed a process model in 1985 for characterizing hierarchical levels of fusion processing, categorized fusion functions, and candidate algorithm approaches, and is the most widely used system for categorizing data fusion-related functions \cite{JDLFusion}. The model is illustrated in figure \ref{fig:jdl_fusion}.
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{fig/JDLfusion.png}
    \caption{The Joint Directors of Laboratories data fusion model.}
    \label{fig:jdl_fusion}
\end{figure}
The JDL model defines the fusion levels as follows:
\begin{itemize}
    \item Level 0: \textit{Subobject assessment}. Preconditioning data to correct biases, perform spatial and temporal alignment, and standardize inputs.
    \item Level 1: \textit{Object assessment}. Association of data (including products of prior fusion) to estimate an object or entitys position, kinematics, or attributes (including identity).
    \item Level 2: \textit{Situation assessment}. Aggregation of objects/events to perform relational analysis and estimation of their relationships in the context of the operational environment.
    \item Level 3: \textit{Impact assessment}. Projection of the current situation to perform event prediction, threat intent estimation, own force vulnerability, and consequence analysis.
    \item Level 4: \textit{Process refinement}. Evaluation of the ongoing fusion process to provide user advisories and adaptive fusion control or to request additional sensor/source data.
\end{itemize}
The models different levels do not represent a process flow, but rather provides a structured and integrated view on the complete functional chain from distributed sensors, data bases, and human reports to the users and their option to act including various feedback loops at different levels. Although the model was developed with military applications in mind, it remains valid in civilian applications. Koch \cite{sensorFusion1} identifies two characteristic features of sensor data fusion:
\begin{enumerate}
    \item The available sensor data and context knowledge to be fused typically provide incomplete and imperfect pieces of information. The reasons for this are manifold, and are unavoidable in real-world applications. For dealing with these deficiencies, sophisticated mathematical methodologies and reasoning formalisms are applied.
    \item Sensor data fusion is closely related to the practical design of surveillance and reconnaissance components for information systems. In implementing fundamental theoretical concepts, a systematic way of finding reasonable compromises between mathematical exactness and pragmatic realization issues, as well as suitable approximation methodologies are therefore inevitable. System aspects such as robustness and reliability even in case of unforeseeable nuisance phenomena, priority management, and graceful degradation are of particular importance in view of practicability.
\end{enumerate}This project aims to prepare the processing pipeline leading to fusion of measurements from a lidar with measurements from a camera. A lidar provides excellent range information, but has limits with regard to object recognition. A camera, on the other hand, is good for object recognition but has limits to the high resolution range information. The two sensors exhibit complementary properties, and combining (fusing) the measurements from both sensors can achieve more specific inferences about the surroundings than using a single, independent sensor. In order to associate measurements from the lidar and the camera to the external world, fusion with an INS is necessary, to be able to associate the (moving) coordinate frames of the sensors with a common world frame. Moreover, to be able to associate measurements from one sensor with the other, they need to be synchronized in time (temporally calibrated).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computer Vision}
Computer vision is a field of engineering and science concerned with extracting useful information from images. This has proved to be a challenging task, and it is still today an active field of research. Visual data is very complex, and the same object represented by two different images could be perceived very differently by a computer based on variations such as changes in illumination, partial occlusion of objects, changes in orientation, deformation and so on. Such variations are illustrated in figure \ref{fig:cv_challenges}. Despite these challenges, the recent advancements made within the field of deep learning, and deep convolutional neural nets in particular, has significantly improved the ability of computers to recognize objects in images. The ImageNet Large Scale Visual Recognition Challenge has been run annually since 2010, and is a benchmark in object category classification and detection on hundreds of object categories and millions of images \cite{ILSVRC15}. The results of the top 5 classification errors from the challenges up until 2016 is shown in figure \ref{fig:imagenet_top_5}. 
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{fig/challenges.jpeg}
    \caption{Challenges related to object classification in images.\protect\footnotemark}
    \label{fig:cv_challenges}
\end{figure}
\footnotetext{Image courtesy: http://cs231n.github.io/classification/}
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{fig/imagenet_top_5.png}
    \caption{ImageNet top 5 errors over time.\protect\footnotemark}
    \label{fig:imagenet_top_5}
\end{figure}
\footnotetext{Image courtesy: https://www.dsiac.org/resources/journals/dsiac/winter-2017-volume-4-number-1/real-time-situ-intelligent-video-analytics}
As seen in figure \ref{fig:imagenet_top_5}, deep learning algorithms dominate the field of object category classification and detection, even outperforming humans with state-of-the-art deep convolutional neural nets. This section will introduce some necessary basic image processing operations, and give a brief introduction to convolutional neural nets used for image object classification and detection.
\subsection{Spatial Image Filtering}
The term spatial in spatial image filtering refer to the image plane itself, and spatial image processing involves direct manipulation of the pixels themselves in the image, in contrast to other methods operating in a transform domain such as the frequency plane. A spatial filter consists of a \textit{kernel} (also called a mask, neighborhood, template or window) (typically rectangular), and a \textit{predefined operation} that is performed on the pixels in the image encompassed by the neighborhood \cite{digImage}. Filtering the image creates a new pixel with coordinates equal to the center of the neighborhood, with value given by the predefined operation performed on the pixels in the neighborhood. Spatial filtering of an image of size $M\times N$ with a filter of size $m\times n$ is given by
\begin{equation}
\label{eq:spatial_filt}
    g(x,y)=w(x,y)*f(x,y)=\sum^{a}_{s=-a}\sum^{b}_{t=-b}w(s,t)f(x-s,y-t)
\end{equation}
where $a=\frac{m-1}{2}$ and $b=\frac{n-1}{2}$, $w(s,t)$ is the filter kernel, with its center at $w(0,0)$ aligned with the pixel at location $(x,y)$. $g(x,y)$ is the response of the filter at location $(x,y)$. $*$ denotes the convolution operator. $x$ and $y$ are varied such that each pixel in the image is visited. In other words, spatial filtering of an image with a kernel of size $m\times n$ corresponds to convolving the image with the kernel. The filter and the corresponding image pixels for a given $(x,y)$ are illustrated in figure \ref{fig:spatial_filter}. The filter kernel used in this illustration is $3\times3$ pixels.
\begin{figure}[H]
	\centering
	\includegraphics[width=.8\linewidth]{fig/digimageproc.png}
	\caption{}
	\label{fig:spatial_filter}
\end{figure}
Convolution between the image and the filter kernel corresponds to correlating the image and the filter kernel rotated 180 degrees.
\subsection{Convolutional Neural Nets for Object Recognition}
A convolutional neural net, abbreviated CNN, is a neural network designed for object recognition in images. Since their introduction by LeCun \textit{et al}. \cite{lecun-89e} in the late 1980's, convolutional neural nets have shown excellent performance at tasks such as hand-written digit classification and face detection. In 2012, Krizhevsky \textit{et al}. \cite{NIPS2012_4824} won the ImageNet 2012 classification benchmark with their deep convolutional neural network, dubbed AlexNet, achieving an top-5 test error rate of 15.3\%, compared with the 2nd place result of 26.2\%. Since then, the ImageNet classification benchmark has been dominated by deep convolutional nets. The dramatic improvement in performance can be attributed to several factors:
\begin{enumerate}
	\item The availability of very large datasets, with millions of labeled examples.
	\item The adaptation of powerful GPU processing implementations, making training of very large models tractable.
	\item Better model regularization strategies, such as dropout, to prevent overfitting \cite{dropout}.
\end{enumerate}
In a classifier using classical computer vision techniques, features are extracted from an image, for example using a histogram of oriented gradients, and the extracted features are subsequently used to train a classifier such as a support-vector machine. In a convolutional neural network, on the other hand, the features are hidden, and feature extraction and classification is performed in a single pipeline. Features in a CNN are generated via one or several layers of filter convolutions, which generates a set of abstract sub-images from the input image. For the user, a CNN classifier appears as a "black box" where the internal workings of the feature extraction and classification are largely hidden. In order to understand how a convolutional neural net works, some basic concepts need to be introduced. The following sections first introduce the neuron and the concept of a neural network followed by a short discussion on how they are trained, then a brief introduction to convolutional neural nets will be given. The derivations and notation is largely based on that given in \cite{machine_learning}.
\subsubsection{Neural Networks}
In linear regression, a model can be considered as a linear combination of fixed nonlinear functions of the input variables, on the form
\begin{equation}
y(\mathbf{x,w})=w_0+\sum_{j=1}^{n-1}w_j\phi_j(\mathbf{x})
\end{equation}
where $\phi_j(\mathbf{x})$ are known as the nonlinear \textit{basis functions}. A neural network utilized for regression and classification, on the other hand, can be thought of as a combination of basis functions in parametric form, where the parameters of the basis functions are adapted during training. The term \textit{neural network} has its origins in attempts to find mathematical representations of information processing in biological systems. The basic building block of a neural network is the \textit{neuron}, or \textit{perceptron}, illustrated in figure \ref{fig:neuron}.
\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{.5\textwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/neural.png}
		\caption{A neuron, with its inputs,\\ bias and output.}
		\label{fig:neuron}
	\end{subfigure}%
	\begin{subfigure}[b]{.5\textwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/layers.png}
		\caption{A simple neural network.}
		\label{fig:layers}
	\end{subfigure}
	\caption{A neuron, alongside a simple neural network with a single hidden layer.}
	\label{fig:neural_net}
\end{figure}
The basic neural network can be described as a series of functional transformations. First $M$ linear combinations of the input variables $x_1,\dots,x_D$ are formed, as
\begin{equation}
a_j = b_j^{1}+\sum_{i=1}^{D}w_{ji}^{1}x_i
\end{equation}
where $j=1,\dots,M$, $b_j^1$ is a bias, the parameters $w_{ji}^1$ are the weights, and the superscript 1 indicates that the corresponding parameters are the parameters of the first layer in the network. The result of this weighted sum of the inputs, $a_j$ is known as the activation. Each $a_j$ is then transformed via a nonlinear, differentiable activation function, $z_j\sigma(a_j)$, which is then input to the next layer in the network. Figure \ref{fig:neuron} illustrates this process for a single neuron. The quantities $z_j$ are referred to as \textit{hidden units}, and the corresponding layers are referred to as \textit{hidden layers}. The nonlinear functions $\sigma(\cdot)$ are generally chosen to be sigmoidal functions such as the logistic sigmoid ($\sigma(a)=\frac{1}{1+\exp(-a)}$) or the hyperbolic tangent function ($\tanh(a)$). The outputs $z_j$ are again linearly combined to give output unit activations as
\begin{equation}
a_k = b_k^2+\sum_{j=1}^{M}w_{kj}^2z_j
\end{equation} 
where $k=1,\dots,M$ and $K$ is the total number of outputs. This corresponds to the second layer of the network. Lastly, the output unit activations are transformed using an appropriate activation function to give a set of network outputs $y_k$. A simple network with a single hidden layer is illustrated in figure \ref{fig:layers}. The choice for the output activation function is largely determined by the nature of the data and the assumed distribution of the target variables. For binary classification problems, the logistic sigmoid function is commonly used.
\subsubsection{Loss Function and Back Propagation}
\subsubsection{Convolutional Neural Nets}


\cleardoublepage