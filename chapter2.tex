%===================================== CHAP 3 =================================

\chapter{Background Theory}
In this chapter \todo{introduser kapittel}
\section{Sensor Fusion}
\label{section:fusion}
Robots and autonomous vehicles, be it a car or a ship, operate in unstructured environments, environments that are unpredictable. While robots working on a assembly line work in very structured, predictable environments, a vessel attempting to atonomously navigate a busy shipping lane has to rely on sensor inputs in order to make sense of its environment, and navigate safely. In sensing its environment, a vessel might use various sensors, such as radar(s), cameras, sonars or laser range finders. The different sensors typically give a incomplete and imperfect view of the surrounding world. Sensor data fusion is the process of combining such mutually complementary sensor information in such a way that a better understanding of the surroundings can be achieved \cite{sensorFusion1}.
The U.S. Joint Directors of Laboratories (JDL) Data Fusion Working Group developed a process model in 1985 for characterizing hierarchical levels of fusion processing, categorized fusion functions, and candidate algorithm approaches, and is the most widely used system for categorizing data fusion-related functions \cite{JDLFusion}. The model is illustrated in figure \ref{fig:jdl_fusion}.
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{fig/JDLfusion.png}
    \caption{The Joint Directors of Laboratories data fusion model.}
    \label{fig:jdl_fusion}
\end{figure}
The JDL model defines the fusion levels as follows:
\begin{itemize}
    \item Level 0: \textit{Subobject assessment}. Preconditioning data to correct biases, perform spatial and temporal alignment, and standardize inputs.
    \item Level 1: \textit{Object assessment}. Association of data (including products of prior fusion) to estimate an object or entitys position, kinematics, or attributes (including identity).
    \item Level 2: \textit{Situation assessment}. Aggregation of objects/events to perform relational analysis and estimation of their relationships in the context of the operational environment.
    \item Level 3: \textit{Impact assessment}. Projection of the current situation to perform event prediction, threat intent estimation, own force vulnerability, and consequence analysis.
    \item Level 4: \textit{Process refinement}. Evaluation of the ongoing fusion process to provide user advisories and adaptive fusion control or to request additional sensor/source data.
\end{itemize}
The models different levels do not represent a process flow, but rather provides a structured and integrated view on the complete functional chain from distributed sensors, data bases, and human reports to the users and their option to act including various feedback loops at different levels. Although the model was developed with military applications in mind, it remains valid in civilian applications. Koch \cite{sensorFusion1} identifies two characteristic features of sensor data fusion:
\begin{enumerate}
    \item The available sensor data and context knowledge to be fused typically provide incomplete and imperfect pieces of information. The reasons for this are manifold, and are unavoidable in real-world applications. For dealing with these deficiencies, sophisticated mathematical methodologies and reasoning formalisms are applied.
    \item Sensor data fusion is closely related to the practical design of surveillance and reconnaissance components for information systems. In implementing fundamental theoretical concepts, a systematic way of finding reasonable compromises between mathematical exactness and pragmatic realization issues, as well as suitable approximation methodologies are therefore inevitable. System aspects such as robustness and reliability even in case of unforeseeable nuisance phenomena, priority management, and graceful degradation are of particular importance in view of practicability.
\end{enumerate}This project aims to prepare the processing pipeline leading to fusion of measurements from a lidar with measurements from a camera. A lidar provides excellent range information, but has limits with regard to object recognition. A camera, on the other hand, is good for object recognition but has limits to the high resolution range information. The two sensors exhibit complementary properties, and combining (fusing) the measurements from both sensors can achieve more specific inferences about the surroundings than using a single, independent sensor. In order to associate measurements from the lidar and the camera to the external world, fusion with an INS is necessary, to be able to associate the (moving) coordinate frames of the sensors with a common world frame. Moreover, to be able to associate measurements from one sensor with the other, they need to be synchronized in time.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computer Vision}
Computer vision is a field of engineering and science concerned with extracting useful information from images. This has proved to be a challenging task, and it is still today an active field of research. Visual data is very complex, and the same object represented by two different images could be perceived very differently by a computer based on variations such as changes in illumination, partial occlusion of objects, changes in orientation, deformation and so on. Such variations are illustrated in figure \ref{fig:cv_challenges}. Despite these challenges, the recent advancements made within the field of deep learning, and deep convolutional neural nets in particular, has significantly improved the ability of computers to recognize objects in images. The ImageNet Large Scale Visual Recognition Challenge has been run annually since 2010, and is a benchmark in object category classification and detection on hundreds of object categories and millions of images \cite{ILSVRC15}. The results of the top 5 classification errors from the challenges up until 2016 is shown in figure \ref{fig:imagenet_top_5}. 
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{fig/challenges.jpeg}
    \caption{Challenges related to object classification in images.\protect\footnotemark}
    \label{fig:cv_challenges}
\end{figure}
\footnotetext{Image courtesy: http://cs231n.github.io/classification/}
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{fig/imagenet_top_5.png}
    \caption{ImageNet top 5 errors over time.\protect\footnotemark}
    \label{fig:imagenet_top_5}
\end{figure}
\footnotetext{Image courtesy: https://www.dsiac.org/resources/journals/dsiac/winter-2017-volume-4-number-1/real-time-situ-intelligent-video-analytics}
As seen in figure \ref{fig:imagenet_top_5}, deep learning algorithms dominate the field of object category classification and detection, even outperforming humans with state-of-the-art deep convolutional neural nets. This section will introduce some necessary basic image processing operations, and give a brief introduction to convolutional neural nets used for image object classification and detection.
\subsection{Spatial Image Filtering}
The term spatial in spatial image filtering refer to the image plane itself, and spatial image processing involves direct manipulation of the pixels themselves in the image, in contrast to other methods operating in a transform domain such as the frequency plane. A spatial filter consists of a \textit{kernel} (also called a mask, neighborhood, template or window) (typically rectangular), and a \textit{predefined operation} that is performed on the pixels in the image encompassed by the neighborhood \cite{digImage}. Filtering the image creates a new pixel with coordinates equal to the center of the neighborhood, with value given by the predefined operation performed on the pixels in the neighborhood. Spatial filtering of an image of size $M\times N$ with a filter of size $m\times n$ is given by
\begin{equation}
\label{eq:spatial_filt}
    g(x,y)=w(x,y)*f(x,y)=\sum^{a}_{s=-a}\sum^{b}_{t=-b}w(s,t)f(x-s,y-t)
\end{equation}
where $a=\frac{m-1}{2}$ and $b=\frac{n-1}{2}$, $w(s,t)$ is the filter kernel, with its center at $w(0,0)$ aligned with the pixel at location $(x,y)$. $g(x,y)$ is the response of the filter at location $(x,y)$. $*$ denotes the convolution operator. $x$ and $y$ are varied such that each pixel in the image is visited. In other words, spatial filtering of an image with a kernel of size $m\times n$ corresponds to convolving the image with the kernel. The filter and the corresponding image pixels for a given $(x,y)$ are illustrated in figure \ref{fig:spatial_filter}. The filter kernel used in this illustration is $3\times3$ pixels.
\begin{figure}[H]
	\centering
	\includegraphics[width=.8\linewidth]{fig/digimageproc.png}
	\caption{}
	\label{fig:spatial_filter}
\end{figure}
Convolution between the image and the filter kernel corresponds to correlating the image and the filter kernel rotated 180 degrees.
\subsection{Convolutional Neural Nets for Object Recognition}
A convolutional neural net, abbreviated CNN, is a neural network designed for object recognition in images. Since their introduction by LeCun \textit{et al}. \cite{lecun-89e} in the late 1980's, convolutional neural nets have shown excellent performance at tasks such as hand-written digit classification and face detection. In 2012, Krizhevsky \textit{et al}. \cite{NIPS2012_4824} won the ImageNet 2012 classification benchmark with their deep convolutional neural network, dubbed AlexNet, achieving an top-5 test error rate of 15.3\%, compared with the 2nd place result of 26.2\%. Since then, the ImageNet classification benchmark has been dominated by deep convolutional nets. The dramatic improvement in performance can be attributed to several factors \cite{DBLP:journals/corr/ZeilerF13}:
\begin{enumerate}
	\item The availability of very large datasets, with millions of labeled examples.
	\item The adaptation of powerful GPU processing implementations, making training of very large models tractable.
	\item Better model regularization strategies, such as dropout, to prevent overfitting \cite{dropout}.
\end{enumerate}
In a classifier using classical computer vision techniques, features are extracted from an image, for example using a histogram of oriented gradients, and the extracted features are subsequently used to train a classifier such as a support-vector machine. In a convolutional neural network, on the other hand, the features are hidden, and feature extraction and classification is performed in a single pipeline. Features in a CNN are generated via one or several layers of filter convolutions, which generates a set of abstract sub-images from the input image. For the user, a CNN classifier appears as a "black box" where the internal workings of the feature extraction and classification are largely hidden. In order to understand how a convolutional neural net works, some basic concepts need to be introduced. The following sections first introduce the neuron and the concept of a neural network followed by a short discussion on how they are trained, then a brief introduction to convolutional neural nets will be given. The derivations and notation is largely based on that given in \cite{machine_learning} for the conventional neural network, while the convolutional neural network part is based on \cite{Goodfellow-et-al-2016}.
\subsubsection{Neural Networks}
In linear regression, a model can be considered as a linear combination of fixed nonlinear functions of the input variables, on the form
\begin{equation}
y(\mathbf{x,w})=w_0+\sum_{j=1}^{n-1}w_j\phi_j(\mathbf{x})
\end{equation}
where $\phi_j(\mathbf{x})$ are known as the nonlinear \textit{basis functions}. A neural network utilized for regression and classification, on the other hand, can be thought of as a combination of basis functions in parametric form, where the parameters of the basis functions are adapted during training. The term \textit{neural network} has its origins in attempts to find mathematical representations of information processing in biological systems. The basic building block of a neural network is the \textit{neuron}, or \textit{perceptron}, illustrated in figure \ref{fig:neuron}.
\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{.5\textwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/neural.png}
		\caption{A neuron, with its inputs,\\ bias and output.}
		\label{fig:neuron}
	\end{subfigure}%
	\begin{subfigure}[b]{.5\textwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/layers.png}
		\caption{A simple neural network.}
		\label{fig:layers}
	\end{subfigure}
	\caption{A neuron, alongside a simple neural network with a single hidden layer.}
	\label{fig:neural_net}
\end{figure}
The basic neural network can be described as a series of functional transformations. First $M$ linear combinations of the input variables $x_1,\dots,x_D$ are formed, as
\begin{equation}
a_j = b_j^{1}+\sum_{i=1}^{D}w_{ji}^{1}x_i
\end{equation}
where $j=1,\dots,M$, $b_j^1$ is a bias, the parameters $w_{ji}^1$ are the weights, and the superscript 1 indicates that the corresponding parameters are the parameters of the first layer in the network. The result of this weighted sum of the inputs, $a_j$ is known as the activation. Each $a_j$ is then transformed via a nonlinear, differentiable activation function, $z_j\sigma(a_j)$, which is then input to the next layer in the network. Figure \ref{fig:neuron} illustrates this process for a single neuron. The quantities $z_j$ are referred to as \textit{hidden units}, and the corresponding layers are referred to as \textit{hidden layers}. The nonlinear functions $\sigma(\cdot)$ are generally chosen to be sigmoidal functions such as the logistic sigmoid function
\begin{equation}
\sigma(a)=\frac{1}{1+\exp(-a)}
\end{equation}
or the hyperbolic tangent function ($\tanh(a)$). The outputs $z_j$ are again linearly combined to give output unit activations as
\begin{equation}
a_k = b_k^2+\sum_{j=1}^{M}w_{kj}^2z_j
\end{equation} 
where $k=1,\dots,M$ and $K$ is the total number of outputs. This corresponds to the second layer of the network. Lastly, the output unit activations are transformed using an appropriate activation function to give a set of network outputs $y_k$. A simple network with a single hidden layer is illustrated in figure \ref{fig:layers}. The choice for the output activation function is largely determined by the nature of the data and the assumed distribution of the target variables. For binary classification problems, the logistic sigmoid function is commonly used \cite{machine_learning}.
\subsubsection{Convolutional Neural Nets}
A convolutional neural net or ConvNet, abbreviated CNN, is similar to ordinary neural networks in that they are made up of neurons that have learnable biases and weights. What distincts it from ordinary neural nets is that it assumes that the input data has a grid-like topology, such as an image being a 3D grid of pixels, where the depth is represented by the color channels in the image. As the name implies, the CNN employs convolution in one or more of the layers in the network, and the layers have neurons arranged in 3 dimensions. A traditional neural network use matrix multiplication by a matrix of parameters, with a separate parameter describing the interactions between each input unit and each output unit, meaning that every input unit interacts with every output unit. CNNs however, have \textit{sparse} interactions, accomplished by using kernels which are smaller than the input for holding the weights, which are convolved with the input. This drastically reduces the parameters in the network compared to the traditional network using matrix multiplication. If there are $m$ inputs and $n$ outputs, the matrix multiplication approach requires $m\times n$ parameters. For an input such as an image, which can have thousands or millions of pixels, the number of parameters needed become very large. In a CNN, the number of connections for each output is limited by the kernel size and number of kernels \cite{Goodfellow-et-al-2016}. 

CNNs also employ parameter sharing, where the same parameter is used for more than one output in the model. This is accomplised by the convolution operation, where each member of the kernel is used at every position of the input. This means that rather than learning a separate set of parameters for every location, only one set is learned, further reducing the number of parameters needed in the model. A simple representation of a layer in a convolutional neural net is shown in figure \ref{fig:conv}. The neurons (indicated by white circles) arranged in depth in the output activation layer are connected to the same inputs through different kernels, while all neurons at the same depth share kernel parameters. As indicated in the figure, a single layer may consist of several filter kernels.
\begin{figure}[H]
	\centering
	\includegraphics[width=.7\linewidth]{fig/conv_simple.png}
	\caption{A CNN layer. The 3D input volume is transformed into a 3D volume of neuron activations.}
    \label{fig:conv}
\end{figure}
A typical layer of a CNN consists of three stages; convolution, detection and pooling, illustrated in figure \ref{fig:conv_layer} \cite{Goodfellow-et-al-2016}.
\begin{figure}[H]
	\centering
	\includegraphics[width=.4\linewidth]{fig/conv.png}
	\caption{The three stages of a convolution layer.}
	\label{fig:conv_layer}
\end{figure}
The convolution stage performs several convolutions in parallel to produce a set of linear activations. At the next stage, the linear activations are run through a nonlinear activation function, in a similar fashion as for the traditional neural network. In the pooling stage, a \textit{pooling function} is used to modify the output layer, replacing the output of the net by a summary statistic of the nearby outputs. As an example, the max pooling operation reports the maximum output within a rectangular region. Other examples are the average of a rectangular region, or a weighted average based on the distance from the central pixel.

A convolutional layer can be described by four \textit{hyperparameters}. The term \textit{hyperparameter} is used to distinguish them from the model parameters (weights and biases), and they are not subject to optimization. The four hyperparameters are
\begin{enumerate}
	\item The number of kernels, $K$.
	\item The spatial extent of the filter kernel, $F$.
	\item The stride $S$, the number of pixel displacements for each calculation.
	\item The amount of zero padding to the brim of the image, $P$.
\end{enumerate}
For a given input image of size $\begin{bmatrix}
W_1\times H_1\times D_1
\end{bmatrix}$ the spatial size of the output is 
\begin{equation}
\begin{split}
W_2&=1+(W_1-F+2P)/S\\
H_2&=1+(H_1-F+2P)/S
\end{split}
\end{equation}
Convolving the input with $K$ kernels thus leads to the size of the output volume as $
W_2\times H_2\times K$. Each filter kernel extends through the depth of the input, with dimensions $F\times F\times D_1$, making the total number of parameters for a single layer $K\cdot F\cdot F\cdot D_1$.
\subsubsection{Training a Neural Net}
\label{section:training}
For a neural network used in classification of a input, the training of the network consists of minimizing the \textit{error function}, which is a function of the weights of the network, $\mathbf{w}$. For $K$ separate binary classifications, a network with $K$ outputs can be used, where each output has a \textit{normalized softmax} activation function \cite{machine_learning}, given as
\begin{equation}
\sigma(z_k) = \frac{e^{z_k}}{\sum_{i=1}^{K}e^{z_i}}
\end{equation} 
Associated with each output is a binary class label, $t_k\in\{0,1\},\quad k=1,\dots,K$. Assuming the class labels are independent, the conditional distribution of the targets is \cite{machine_learning}
\begin{equation}
p(\mathbf{t}|\mathbf{x},\mathbf{w})=\prod_{k=1}^{K}y_k(\mathbf{x},\mathbf{w})^{t_k}[1-y_k(\mathbf{x},\mathbf{w})^{1-t_k}]
\end{equation}
Taking the negative logarithm of the corresponding likelihood function gives the error function
\begin{equation}
\label{eq:error_nn}
E(\mathbf{w})=-\sum_{n=1}^{N}\sum_{k=1}^{K}\{t_{nk}\ln y_{nk}+(1-t_{nk})\ln(1-y_{nk})\}
\end{equation}
where $y_{nk}$ denotes $y_k(\mathbf{x}_n,\mathbf{w})$. Training the network consists of finding a weight vector $\mathbf{w}$ that minimizes equation \ref{eq:error_nn}. This is done by taking small steps in weight space from $\mathbf{w}$ to $\mathbf{w}+\delta\mathbf{w}$ such that the gradient of the error function vanishes, minimizing the error function:
\begin{equation}
\label{eq:error_gradient}
\nabla E(\mathbf{w})=0
\end{equation}
The error function has a nonlinear dependence on the weights and biases, and so there may be many local minima where the gradient of the error function is zero. An analytical solution of equation \ref{eq:error_gradient} is typically unobtainable, so the typical approach is to use iterative numerical procedures \cite{machine_learning}. One approach is to use a combination of error backpropagation and stochastic gradient descent, where the error gradient calculated at the output is propagated back through the network, giving the partial derivative of the error for each hidden unit. In stochastic gradient descent, the weights are updated based on one data point at a time, according to 
\begin{equation}
w_{ji}^{\l}\rightarrow w_{ji}^{\l}-\eta \frac{\partial E_n(\mathbf{w})}{\partial w_{ji}^{l}}
\end{equation}
where the parameter $\eta > 0$ is known as the \textit{learning rate}, which is a parameter provided by the user, and $E_n(\mathbf{w})$ is the error function for a single data point. For a more detailed explanation of neural network training the reader is referred to \cite{machine_learning}, in particular chapters 5.2 and 5.3.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Robot Operating System}
A brief introduction to ROS will be given in this section, based on the introduction given in \textit{Programming Robots with ROS} by Quigley et al. \cite{programming_ros}. ROS, which is an abbreviation for Robot Operating System, is not, as the name might imply, a operating system, but a framework running on top of a traditional OS. The framework of ROS is based on several philosophical aspects:
\begin{itemize}
	\item \textit{Peer to peer}: A ROS system consists of several small programs (called nodes) connected to each other, continuously exchanging messages. Messages travel directly from one node to another.
	\item \textit{Tools-based}: Complex ROS systems can be created from many small, generic programs. ROS does not have a integrated development and runtime environment, and tasks such as (but not limited to) navigating the source code tree, visualizing the system interconnections, generating documentation, and logging data are performed by separate programs.
	\item \textit{Multilingual}: ROS software modules can be written in any language for which a \textit{client library} has been written. Client libraries exist for C++, Python, LISP, Java, JavaScript, MATLAB, Ruby, Haskell, R, and others. This provides flexibility for the programmer, in that he or she can choose the language they are most familiar with, or is best suited for the task, when creating new functionality. ROS achieves this multilinguality by enforcing a convention for serializing messages being passed between nodes.
	\item \textit{Thin}: ROS conventions encourage developers to create standalone libraries and then wrap those libraries so that they can send and receive messages to and from other ROS modules. This allows reuse of software outside ROS, and simplifies automated testing using continuous integration tools.
	\item \textit{Free and open source}: The core of ROS is released under the BSD license \cite{BSD}, which allows commercial and noncommercial use.
\end{itemize}
In ROS, \lstinline[basicstyle=\ttfamily]{roscore} is a service that provides connection information to nodes so that they can transmit messages to and from one another. Each node connects to \lstinline[basicstyle=\ttfamily]{roscore} on startup to register details of the messages it publishes, and the details of the messages to which it wishes to subscribe. \lstinline[basicstyle=\ttfamily]{roscore} is not a server in the traditional client/server sense, all messages are sent peer-to-peer between nodes. The \lstinline[basicstyle=\ttfamily]{roscore} service is only used by nodes to know where to find their peers. Due to this fact, every ROS system needs a running \lstinline[basicstyle=\ttfamily]{roscore}. Upon startup, every ROS node expects its process to have an environment variable \lstinline[basicstyle=\ttfamily]{ROS_MASTER_URI}, containing a string in the format \lstinline[basicstyle=\ttfamily]{http://hostname:portnumber}, to tell it where the \lstinline[basicstyle=\ttfamily]{roscore} service is running, and which port it is accessible on. The connection between nodes, and between individual nodes and \lstinline[basicstyle=\ttfamily]{roscore} is illustrated in figure \ref{fig:roscore}.
\begin{figure}[H]
	\centering
	\includegraphics[width=.4\linewidth]{fig/roscore.png}
	\caption{Roscore and its connection to other nodes in the system.}
	\label{fig:roscore}
\end{figure}
The ephemeral connection between nodes and \lstinline[basicstyle=\ttfamily]{roscore} is illustrated by dashed lines, indicating periodic calls to \lstinline[basicstyle=\ttfamily]{roscore} to find peers, while the peer-to-peer message passing is illustrated by the solid line between the nodes.
\cleardoublepage