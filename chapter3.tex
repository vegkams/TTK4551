%===================================== CHAP 2 =================================
\graphicspath{{fig/}}
\chapter{Sensor Models}
\section{Image Formation}
Camera systems have found a wide array of applications since its invention, and are today widely used in industry. Application areas range from surveillance (CCTV), to image acquisition in scientific exploration of the earth, oceans and space. In recent years, there has been a drive towards utilizing cameras for automatic object detection and recognition, made possible through advances in computing power and algorithms. There is a drive towards more and more autonomous operation of vehicles, and camera systems play an integral part in the sensing of the environment necessary for safe autonomous operation. This section presents how an image is formed in a digital camera, and how to relate image coordinates to world coordinates through transformations. At the end of this section a methodology widely used for camera calibration is presented. The theory presented in this section is by no means exhaustive, for a more in-depth treatment of the theory behind image formation see \cite{modernCV,CValg,digImage}.
\subsection{The Pinhole Camera Model}
\label{section:pinhole}
Pinhole camera, thin lens model, thick lens model, refraction, snell's law, paraxial (or first-order) geometric optics, depth of field/depth of focus, field of view, abberations... \smallskip \\
The pinhole camera model was first proposed by Brunelleschi in the early fifteenth century \cite{modernCV}. Consider a rectangular box with a translucent plate at one end. At the opposite end there is a small pinhole, small enough so that exactly one ray of light would pass through the translucent plate, the pinhole, and some scene point. The distance between the pinhole and the translucent plate is called the \textit{focal length}, denoted \textit{f}. This is of course impossible in reality, as the pinhole will have a finite, though small size. As such, each point in the image plane will collect light from a cone of rays. A real camera will also be equipped with lenses, which complicates the simple model of rays passing through a hole. Still, the pinhole camera model is mathematically convenient, and often provides an acceptable approximation of the imaging process. The pinhole perspective projection model is illustrated in figure \ref{fig:pinhole_model}.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{pinhole.png}
    \caption{The pinhole camera model}
    \label{fig:pinhole_model}
\end{figure}
The pinhole perspective creates inverted images, so it is sometimes convenient to consider a virtual image plane lying in front of the pinhole, at the same distance from the pinhole as the actual image plane. The virtual image is not inverted, but otherwise identical to the actual image. There are some effects of this perspective projection which deserves some consideration. One such effect is the fact that the apparent size of objects depend on their distance from the pinhole. Consider two objects which are identical. If one object is placed farther away from the pinhole relative to the other, it will appear smaller than the closer object. Another effect of perspective projection is parallel lines in appearing to converge on a horizon line in the image plane. These effects are illustrated in figure \ref{fig:perspective}.
\begin{figure}[H]
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\linewidth]{perspective.png}
        \caption{Close objects appear larger than distant ones.}
        \label{fig:perspective1}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\linewidth]{perspective2.png}
        \caption{Parallel lines appear to converge on a horizon line.}
        \label{fig:perspective2}
    \end{subfigure}
    \caption{Perspective effects.}
    \label{fig:perspective}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Digital Image Aquisition}
A digital image is formed by electromagnetic radiation illuminating a sensor arrangement. The electromagnetic radiation gathered by the sensors typically falls within the spectrum of visible light (with a wavelength approximately between 400 and 700 nanometers), but can also be in the infrared or ultraviolet spectrum, or in the X-ray spectrum as is the case for X-ray detectors in medical applications. The sensed wavelength depends on the application, and in the scope of this project only the visible spectrum is considered. For simplicity, the electromagnetic radiation falling within the visible spectrum will simply be adressed as \textit{light}. The images are generated by the combination of an illumination source and the reflection or absorption of energy from that source by the elements of the scene being imaged. A digital camera is a \textit{passive} sensor, as in it detects input generated by the physical environment. Incoming energy is transformed into a voltage by the combination of input electrical power and sensor material that is responsive to the energy being detected. The sensor outputs a voltage waveform as a response, and this voltage is sampled and quantized, producing a digital value.

Individual sensors are arranged in a 2D CCD array. The response of each individual sensor is proportional to the integral of the light energy projected onto the sensor \cite{digImage}. Color images are produced by placing a color filter consisting of red, blue and green in front of the sensor array. The typical color filter setup, known as a Bayer filter \cite{bayer1976color} is shown in figure \ref{fig:ccd_color}.
\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{fig/700px-Bayer_pattern_on_sensor.png}
  \caption{The Bayer filter.}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{fig/375px-Bayer_pattern_on_sensor_profile.png}
  \caption{Profile of sensor.}
  \label{fig:sub2}
\end{subfigure}
\caption{The Bayer filter overlaid a pixel array.\protect\footnotemark}
\label{fig:ccd_color}
\end{figure}
\footnotetext{Images courtesy: https://en.wikipedia.org/wiki/Bayer\_filter}
The voltage output by each pixel may be read and quantized by a ADC producing a digital value, usually in the range 0-255 (8-bit).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Intrinsic and Extrinsic Parameters}
The world and camera coordinate systems are related by a set of physical parameters. These parameters can be separated into \textit{intrinsic} parameters, which relates the image coordinate system to the normalized coordinate system presented in figure \ref{fig:image_coordinates}, and \textit{extrinsic} parameters, which relate the cameras coordinate system to a fixed world coordinate system and specify its position and orientation in space.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{intrinsic.png}
    \caption{Coordinate systems}
    \label{fig:image_coordinates}
\end{figure}
In figure \ref{fig:image_coordinates}, $^nO$ denotes the origin of the normalized image plane, $^iO$ is the origin of the physical image plane, $^wO$ is the origin of the world frame, and $^cO$ is the origin of the camera 3D frame. Since $^cO$, $^c\hat{p}$ and $^cP$ in figure \ref{fig:image_coordinates} are collinear, we have that $\overrightarrow{^cO\hat{p}}=\lambda\overrightarrow{^cOP}$ for some constant $\lambda$ such that
\begin{equation}
\begin{cases}
    \hat{x}=\lambda X \\
    \hat{y}=\lambda Y \Longleftrightarrow \lambda = \frac{\hat{x}}{X}=\frac{\hat{y}}{Y}=\frac{1}{Z}\\
    1 = \lambda Z
\end{cases}
\end{equation}
which gives the following relation between the normalized plane coordinates $^n\hat{p}$ and the coordinates of the point $^cP$
\begin{equation}
\label{eq:perspectiveproj}
    \begin{cases}
    \hat{x}=\frac{X}{Z}\\
    \hat{y}=\frac{Y}{Z}
    \end{cases}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Homogeneous Coordinates and Rigid Transformations}
\label{section:transformations}
Homogeneous coordinates are convenient for representing geometric transformations by a matrix product.
Consider a point $P$ in some right-handed coordinate frame with origin $O$ and the $x$, $y$ and $z$ axis given by the unit vectors $\mathbf{i}$, $\mathbf{j}$ and $\mathbf{k}$ respectively. The point can be specified by
\begin{equation}
    \overrightarrow{OP}=X\mathbf{i}+Y\mathbf{j}+Z\mathbf{k}
\end{equation}
The nonhomogeneous coordinate vector $P$ is the vector $(X, Y, Z)^T$ in $\mathbb{R}^3$, while its homogeneous counterpart is the vector $(X,Y,Z,1)^T$ in $\mathbb{R}^4$. The change of coordinates between two (arbitrary) Euclidean coordinate systems $a$ and $b$ can be represented by a rotation matrix $^a\mathcal{R}_b$ and a translation vector $\mathbf{t}$ in $\mathbb{R}^3$ as
\begin{equation}
    ^a\mathbf{P}=^a\mathcal{R}_b^b\mathbf{P}+\mathbf{t}
\end{equation}
The notation used for the rotation matrix describes the direction of the transformation, where the superscript denotes the resulting coordinate frame, while the subscript denotes the coordinate frame the vector to be transformed is represented in, as $^{\text{to}}\mathcal{R}_{\text{from}}$.
Using homogeneous coordinates, the same transformation can be written as
\begin{equation}
\label{eq:homo_trans}
    ^a\mathbf{P}=\mathcal{T}^b\mathbf{P},\quad \text{where}\quad \mathcal{T}=\begin{bmatrix}^a\mathcal{R}_b & \mathbf{t} \\
    \mathbf{0}^T & 1 \end{bmatrix}
\end{equation}
where $^a\mathbf{P}$ and $^b\mathbf{P}$ are now vectors in $\mathbb{R}^4$. There are several ways to parameterize the rotation matrix $\mathcal{R}$, such as by Euler angles or quaternions. There are many textbooks which go into great detail on parameterizations and properties of rotation matrices, some examples being \cite{modsim}, \cite{kinematics} and \cite{geometry}. Going into such details is beyond the scope of this text.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Intrinsic Parameters}
\label{section:intrinsic_params}
Using homogeneous coordinates, equation \ref{eq:perspectiveproj} can be rewritten as
\begin{equation}
\label{eq:homo}
^n\mathbf{\hat{p}}=\frac{1}{Z}\begin{bmatrix}\mathbf{I}^{3\times3} & \mathbf{0}^{3\times1}\end{bmatrix}^c\mathbf{P}
\end{equation}
where $\mathbf{I}^{3\times3}$ is the 3-by-3 identity matrix, and $^n\mathbf{\hat{p}}\triangleq \begin{bmatrix}\hat{x}&\hat{y}&1\end{bmatrix}^T$ is the homogeneous coordinate vector of the projection $\hat{p}$ of the point $P$ into the normalized image plane, and $^c\mathbf{P}$ is the homogeneous coordinate vector of the point $P$ in the camera coordinate frame. The following section relates the homogeneous coordinates of the normalized image plane to those of the physical image plane. The physical retina of a camera is located at a distance $f\neq1$ from the pinhole (this assumes that the camera is focused at infinity, so that the physical distance from the pinhole to the image plane equals the focal length), and the physical image point $p$ is usually expressed in pixel units. In addition to this, pixels may be rectangular instead of square, introducing two additional scale parameters $k$ and $l$, such that
\begin{equation}
\label{eq:physical_plane_scaled}
    \begin{cases}
    x=kf\frac{X}{Z}=kf\hat{x}\\
    y=lf\frac{Y}{Z}=kf\hat{y}
    \end{cases}
\end{equation}
We have that $f$ represents a distance, in meters say, and a pixel has dimensions $\frac{1}{k}\times\frac{1}{l}$, where $k$ and $l$ are expressed in $\frac{\text{pixel}}{m}$. The parameters can be replaced by a magnification $\alpha=kf$ and $\beta=lf$ expressed in pixels. In general the origin of the physical image plane is in a corner $c$ of the physical image plane (often the upper left corner, but this may vary) and not at it's center, and the center of the CCD matrix typically does not coincide with the image center $c_0$. This adds two more parameters $x_0$ and $y_0$ that define the position of $c_0$ in pixel units in the physical image coordinate system. Equation \ref{eq:physical_plane_scaled} is then replaced by
\begin{equation}
\label{eq:physical_plane_scaled_and_translated}
    \begin{cases}
    x=\alpha\hat{x}+x_0\\
    y=\beta\hat{y}+y_0
    \end{cases}
\end{equation}
This assumes that the angle between the image axes equals exactly 90 degrees. In reality, however, the image plane axes may be skewed, such that the angle between the image plane axes is $\theta\neq90\degree$. In this case, equation \ref{eq:physical_plane_scaled_and_translated} is replaced by
\begin{equation}
\label{eq:physical_plane_scewed}
    \begin{cases}
    x=\alpha\hat{x}-\alpha\cot{\theta}\hat{y}+x_0\\
    y=\frac{\beta}{\sin{\theta}}\hat{y}+y_0
    \end{cases}
\end{equation}
In matrix form, equation \ref{eq:physical_plane_scewed} can be written as
\begin{equation}
\label{eq:calibr}
    \mathbf{p}=\mathcal{K}\mathbf{\hat{p}}, \quad \mathbf{p}=\begin{bmatrix}x\\y\\1\end{bmatrix},\quad \mathcal{K}\overset{\text{def}}{=}\begin{bmatrix}\alpha & -\alpha\cot{\theta} & x_0\\0 & \frac{\beta}{\sin{\theta}} & y_0\\0 & 0 & 1\end{bmatrix}
\end{equation}
The matrix $\mathcal{K}$ is called the \textit{calibration matrix} of the camera. Putting the equations \ref{eq:homo} and \ref{eq:calibr} together, we obtain
\begin{equation}
\label{eq:intrinsics}
    ^i\mathbf{p}=\frac{1}{Z}\mathcal{K}\begin{bmatrix}\mathbf{I}^{3\times3} & \mathbf{0}^{3\times1}\end{bmatrix}^c\mathbf{P}=\frac{1}{Z}\mathcal{M}^c\mathbf{P},\quad \text{where}\quad \mathcal{M}\overset{\text{def}}{=}\begin{bmatrix}\mathcal{K} & \mathbf{0}^{3\times1}\end{bmatrix}
\end{equation}
The parameters $\alpha$, $\beta$, $\theta$, $x_0$ and $y_0$ are called the \textit{intrinsic parameters} of the camera, typically obtained through a calibration procedure like the one described in section \ref{app:cam_calibration}.
\subsubsection{Extrinsic Parameters}

Equation \ref{eq:intrinsics} is written in a coordinate frame rigidly attached to the camera. However, for applications like autonomous vehicles, where the vehicle itself moves in some inertial coordinate system, we can express equation \ref{eq:intrinsics} in some world coordinate system using a rigid homogeneous transformation. The change of coordinates between the camera frame and the world frame can be expressed as presented in equation \ref{eq:homo_trans}:
\begin{equation}
    ^c\mathbf{P}=\begin{bmatrix}\mathcal{R} & \mathbf{t}\\ \mathbf{0}^T & 1\end{bmatrix} {}^w\mathbf{P}
\end{equation}
Here, $^w\mathbf{P}$ represents the point $P$ in some world coordinate frame $w$. Substituting this into equation \ref{eq:intrinsics} yields
\begin{equation}
\label{eq:extrinsics}
    ^i\mathbf{p}=\frac{1}{Z}\mathcal{M}{}^w\mathbf{P},\quad \text{where}\quad \mathcal{M}=\mathcal{K}\begin{bmatrix}\mathcal{R} & \mathbf{t}\end{bmatrix}
\end{equation}
Knowing $\mathcal{M}$ determines the position of the camera's optical center in the world coordinate frame $w$. A rotation matrix $\mathcal{R}$ is defined by three independent parameters (such as Euler angles), and together with the translation vector $\mathbf{t}$ defines six \textit{extrinsic parameters} that define the position and orientation of the camera relative to the world coordinate frame. The depth $Z$ in equation \ref{eq:extrinsics} is dependent on $\mathcal{M}$ and $^w\mathbf{P}$, which can be seen directly from the equation. If we denote the three rows of $\mathcal{M}$ as $\mathbf{m}_1^T$, $\mathbf{m}_2^t$ and $\mathbf{m}_3^T$, it follows that $Z=\mathbf{m}_1^T\cdot {}^w\mathbf{P}$. It's important to note that the matrix $\mathcal{M}$ is only defined up to scale, and multiplying $\mathcal{M}$ by a arbitrary constant $\lambda\neq0$ does not change the resulting image coordinates since
\begin{equation}
\begin{cases}
    {}^i x=\frac{\mathbf{m}_1^T\cdot {}^w\mathbf{P}}{\mathbf{m}_3^T\cdot {}^w\mathbf{P}}\\
    {}^i y=\frac{\mathbf{m}_2^T\cdot {}^w\mathbf{P}}{\mathbf{m}_3^T\cdot {}^w\mathbf{P}}
\end{cases}
\end{equation}
and the constant $\lambda$ cancels out. The perspective projection matrix $\mathcal{M}$ can be written explicitly as a function of its five intrinsic parameters, the rows $\mathbf{r}_1^T$, $\mathbf{r}_2^T$ and $\mathbf{r}_3^T$ of the rotation matrix $\mathcal{R}$ and the coordinates of of the vector $\mathbf{t}=\begin{bmatrix}t_1 & t_2 & t_3\end{bmatrix}^T$ as 
\begin{equation}
    \mathcal{M}=\begin{bmatrix}\alpha\mathbf{r_1}-\alpha\cot{\theta}\mathbf{r}_2^T+x_0\mathbf{r}_3^T & \alpha t_1-\alpha\cot{\theta}t_2+x_0t_3\\ \frac{\beta}{\sin{\theta}}\mathbf{r}_2^T + y_0\mathbf{r}_3^T & \frac{\beta}{\sin{\theta}}t_2+y_0t_3 \\
    \mathbf{r}_3^T & t_3\end{bmatrix}
\end{equation}
The intrinsic parameters are typically found through a calibration routine, while the extrinsic parameters depend on the location of the camera frame relative to the world frame the user chooses. A detailed explanation of camera calibration can be found in appendix \ref{app:cam_calibration}.
%\subsection{Cameras installed on ReVolt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LIDAR %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Lidar}
A lidar is an \textit{active} electro-optical sensor that sends out a laser pulse, and subsequently measures the parameters of the return signal bounced off some object. The lidar is known under several names, some examples being ladar, lidar, LIDAR, LADAR or laser radar. The term lidar is the most common, and will be used throughout this report. Lidars have seen a growing range of applications in recent years, with the current drive towards autonomous, driverless cars as a significant driving factor. In a lidar, a waveform generator generates a laser waveform. Depending on the type of lidar, the setup can include a single laser or a master oscillator with multiple lasers or laser amplifiers. There are many types of lidars, however this report focuses on a particular type of 3D lidar with a rotating detector array, which measures azimuth and elevation angle, and range. 

The measurement is performed by the laser pulse being guided through transmit optics, traversing some medium, typically atmosphere, to a target. The laser pulse bounces off the target, and traverses the media again until receive optics captures the reflected pulse, guiding it to a detector or a detector array \cite{SpieLidar}. The laser and detector arrays rotate, taking multiple measurements as it scans the full 360 degree field of view. The range to a target can be determined based on the travel time of the laser pulse by $R=\frac{c}{2}(t_{rx}-t_{tx})$, where $c$ is the speed of light in the intervening medium, $t_{tx}$ and $t_{rx}$ is the transmission and reception time of the laser pulse, respectively. The laser travels twice the distance to the target between transmission and reception time, hence the resulting range is halved. Relativistic effects are assumed negligible. The range resolution is determined by $\Delta R=\frac{c}{2B}$, where $B$ is the system bandwidth, typically given by the transmit signal bandwidth.

\subsection{Velodyne VLP-16 Lidar Specifications}
The lidar used in this project is the Velodyne VLP-16, which is a small, real-time rotating lidar which steams data over a TCP/IP connection when it is powered up. The lidar has multiple return modes, where it can report either the strongest return signal, the last return signal, or both. The default return mode is to report the strongest return, and this mode is what is used in this project. The Velodyne VLP-16 features 16 laser/detector pairs mounted in a rotating housing, rapidly spinning to produce 360$\degree$ 3D image.\smallskip \\
Some features of the VLP-16 are \cite{velodyne_vlp16}:
\begin{itemize}
	\item Horizontal FOV of 360$\degree$.
	\item Weight of 830 grams.
	\item Rotational speed of 5-20 RPS (adjustable).
	\item Vertical FOV of 30$\degree$ (+15$\degree$ to -15$\degree$).
	\item Vertical angular resolution of 2\degree.
	\item Horizontal angular resolution of 0.1$\degree$-0.4$\degree$.
	\item Range of up to 100 meters (range depends on application).
	\item Accuracy $\pm3$ cm (typical).
	\item 903 nm wavelength laser.
\end{itemize}
The Velodyne VLP-16 is shown in figure \ref{fig:vlp_16}.
\begin{figure}[H]
	\centering
	\includegraphics[width=.7\linewidth]{fig/Velodyne_LiDAR_Puck_VLP-16_Left_900.png}
	\caption{The Velodyne VLP-16 lidar.}
	\label{fig:vlp_16}
\end{figure}
\subsection{Lidar Sensor Model}
A lidar is an optical detector, similar to a digital camera in that it responds to the intensity of the light hitting the detector, generating a voltage equal to the square of the intensity of the impinging light \cite{SpieLidar}. A rotating 3D lidar returns the intensity of the measured signal along with the range calculated from the travel time, as well as the azimuth (rotation) angle and the vertical angle of the point. The lidar has its own reference frame (in sperical coordinates), and it keeps track of its own orientation, giving the azimuth angle of a point relative to some reference angle (typically determined by the manufacturer), and the vertical angle is referenced to the horizontal plane in the lidar frame of reference. The coordinates of a point in the lidar frame therefore consists of two angles, $\alpha$ and $\beta$, as well as the distance $R$ from the lidar to the point. Figure \ref{fig:lidar_spherical} illustrates this.
\begin{figure}[H]
    \centering
    \includegraphics[width=.6\linewidth]{fig/sperical_to_cartesian.png}
    \caption{Lidar spherical coordinate system, with equivalent cartesian coordinates.}
    \label{fig:lidar_spherical}
\end{figure}
In figure \ref{fig:lidar_spherical}, $\alpha$ is the azimuth angle, relative to the y-axis. $\beta$ is the vertical/elevation angle, and $R$ is the distance from the lidar to the point $P$.
From figure \ref{fig:lidar_spherical}, using basic trigonometry, we get that the transformation from spherical to cartesian coordinates is given by
\begin{equation}
\label{eq:spheric_to_cartesian}
    \begin{split}
        X&=P'\sin{\alpha}=R\cos{\beta}\sin{\alpha}\\
        Y&=P'\cos{\alpha}=R\cos{\beta}\cos{\alpha}\\
        Z&=R\sin{\beta}
    \end{split}
\end{equation}
The range returned by the Velodyne VLP-16 lidar is measured along a beam at known azimuth angles $\alpha$, and the 16 laser/detector pairs are mounted vertically at equidistant angles from -15$\degree$ to 15$\degree$. When the lidar is rotating at a frequency of 10 Hz the lasers fire at every $0.2\degree$, with a total of $1800$ laser firings for a full rotation. The theoretical maximum number of range measurements for a full $360\degree$ scan is therefore $\frac{360}{0.2}\times16=28800$ measurements, where every single laser beam is reflected and detected at the lidar.
\begin{figure}[H]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.7\linewidth]{fig/lidar_side.png}
		\caption{Side view.}
		\label{fig:sub_lidarframe}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.7\linewidth]{fig/lidar_top.png}
		\caption{Top view.}
		\label{fig:sub_lidarframe2}
	\end{subfigure}
	\caption{The lidar cartesian coordinate frame, with angles from the spherical coordinate frame drawn in.}
	\label{fig:lidar_frames}
\end{figure}

A single reflection from the lidar can be represented in a cartesian coordinate frame centered in the lidar, shown in figure \ref{fig:lidar_frames}, using the transformation given in equation \ref{eq:spheric_to_cartesian} as:
\begin{equation}
\mathbf{P}^{l}_{ij}=\begin{bmatrix}
R_{ij}\cos{\beta_i}\sin{\alpha_j}\\
R_{ij}\cos{\beta_i}\cos{\alpha_j}\\
R_{ij}\sin{\beta_i}
\end{bmatrix}
\end{equation}
Here, $\alpha_j$ is the $j^{\text{th}}$ azimuth angle in the scan, and $\beta_i$ is the angle of the $i^{\text{th}}$ vertical laser. The superscript $l$ denotes the cartesian frame centered in the lidar. $R_{ij}$ is the range measured at the given azimuth and elevation angle. The measurement from the lidar also includes a measure of the returned signal strengt, given as an integer in the range 0-255. If we denote the returned signal strength for the $ij^{\text{th}}$ measurement as $\rho_{ij} = [0, 255]\in \mathbb{N}$, and augment the measurement vector with the returned signal strength, we can write a single measurement as
\begin{equation}
\mathbf{z}^l_{ij}\overset{\text{def}}{=}\begin{bmatrix}
\mathbf{P}^{l}_{ij}\\\rho_{ij}
\end{bmatrix}=\begin{bmatrix}
R_{ij}\cos{\beta_i}\sin{\alpha_j}\\
R_{ij}\cos{\beta_i}\cos{\alpha_j}\\
R_{ij}\sin{\beta_i}\\
\rho_{ij}
\end{bmatrix}
\end{equation}
where the \textit{measurement} $\mathbf{z}^l_{ij}$ denotes the position of the reflection point $\mathbf{P}^{l}_{ij}$ along with the strength of the reflected signal. This model assumes that the range measurement $R_{ij}$ reported by the lidar is exact, as well as that the angles $\beta_i$ and $\alpha_j$ are known with perfect accuracy, which is not the case. There is uncertainty in the measurements, but this is assumed negligible in order to have a tractable measurement model.
\subsection{Lidar Calibration}
In order to use the measurements in conjunction with other measurements, such as those from a camera, they must be transformed to some common coordinate frame. As described in section \ref{section:transformations}, the measurement from the lidar can be transformed to another frame of reference by a rotation followed by a translation. If a navigation system gives the translation $\mathbf{t}^{w}_{wl}$ and the rotation $\mathcal{R}^{w}_{l}$ from the lidar frame to the world frame, the lidar measurement can be written in the world frame as
\begin{equation}
\mathbf{z}^w_{ij} = \begin{bmatrix}
\mathbf{t}^w_{wl}+\mathcal{R}^{w}_{l}\mathbf{P}^{l}_{ij}\\
\rho_{ij}
\end{bmatrix}
\end{equation}
which is the lidar measurement model for a single point reflection, giving the 3D point coordinates alongside the returned signal strength as a $4\times 1$ vector.

The returned signal strength is a measure of the reflectivity of the surface of the object reflecting the laser beam, and the Velodyne VLP-16 lidar is factory-calibrated using commercially available reflectivity standards and retro-reflectors \cite{velodyne_vlp16}. The calibration data is stored in a calibration table within the FPGA in the VLP-16. Figure \ref{fig:lidar_reflectortypes} illustrates the different reflector types used in the calibration.
\begin{figure}
	\centering
	\includegraphics[width=.7\linewidth]{fig/vlp16_calibration.png}
	\caption{Reflector types used in calibrating the VLP-16 lidar. Image taken from \cite{velodyne_vlp16}.}
	\label{fig:lidar_reflectortypes}
\end{figure}
\subsection{The Lidar Point Cloud}
For an entire 360$\degree$ scan, the measurement returned by the lidar can be represented as an aggregation of single point measurements as
\begin{equation}
\mathbf{Z}^w=\begin{bmatrix}
\mathbf{z}^{w}_{11} & \mathbf{z}^{w}_{12} & \dots & \mathbf{z}^{w}_{1j}\\
\mathbf{z}^{w}_{21} & \mathbf{z}^{w}_{22} & \dots & \mathbf{z}^{w}_{2j}\\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{z}^{w}_{i1} & \mathbf{z}^{w}_{i2} & \dots & \mathbf{z}^{w}_{ij}
\end{bmatrix}
\end{equation}
where, as before, $i$ denotes the index of the vertical laser (from 1 to 16 for the velodyne VLP-16), and $j$ is the index of the asimuth angle in the right-open interval $\interval[open right]{0\degree}{360\degree}$. When the Velodyne VLP-16 rotates at 10 Hz, $j$ ranges from 1 to 1800. $\mathbf{Z}^w$ represents a \textit{point cloud} made by a full rotation of the lidar, decomposed in the cartesian world frame.


\cleardoublepage