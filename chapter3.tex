%===================================== CHAP 2 =================================
\graphicspath{{fig/}}
\chapter{Sensor Models}
In this chapter, mathematical models relating measurements from the two exteroceptive sensors used in the project to world coordinates are presented. In order to associate measurements from one sensor to the other, as well as measurements from the sensors with a common world coordinate frame, a mathematical model describing this relationship is needed.
\section{Image Formation}
Camera systems have found a wide array of applications since its invention, and are today widely used in industry. Application areas range from surveillance (CCTV), to image acquisition in scientific exploration of the earth, oceans and space. In recent years, there has been a drive towards utilizing cameras for automatic object detection and recognition, made possible through advances in computing power and algorithms \todo{kilde}. There is a drive towards more and more autonomous operation of vehicles, and camera systems play an integral part in the sensing of the environment necessary for safe autonomous operation \todo{kilde}. This section presents the pinhole camera model, and how to relate image coordinates to world coordinates through transformations. The theory presented in this section is largely based on chapter 1 in \cite{modernCV}.
\subsection{The Pinhole Camera Model}
\label{section:pinhole}
The pinhole camera model was first proposed by Brunelleschi in the early fifteenth century \cite{modernCV}. Consider a rectangular box with a translucent plate at one end. At the opposite end there is a small pinhole, small enough so that exactly one ray of light would pass through the translucent plate, the pinhole, and some scene point. The distance between the pinhole and the translucent plate is called the \textit{focal length}, denoted \textit{f}. This is of course impossible in reality, as the pinhole will have a finite, though small size. As such, each point in the image plane will collect light from a cone of rays. A real camera will also be equipped with lenses, which complicates the simple model of rays passing through a hole. Still, the pinhole camera model is mathematically convenient, and often provides an acceptable approximation of the imaging process. The pinhole perspective projection model is illustrated in figure \ref{fig:pinhole_model}.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{pinhole.png}
    \caption{The pinhole camera model}
    \label{fig:pinhole_model}
\end{figure}
The pinhole perspective creates inverted images, so it is sometimes convenient to consider a virtual image plane lying in front of the pinhole, at the same distance from the pinhole as the actual image plane. The virtual image is not inverted, but otherwise identical to the actual image.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Intrinsic and Extrinsic Parameters}
\label{section:camera_coordinates}
The world and camera coordinate systems are related by a set of physical parameters. These parameters can be separated into \textit{intrinsic} parameters, which relates the image coordinate system to the normalized coordinate system presented in figure \ref{fig:image_coordinates}, and \textit{extrinsic} parameters, which relate the cameras coordinate system to a fixed world coordinate system and specify its position and orientation in space.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{intrinsic.png}
    \caption{Coordinate systems}
    \label{fig:image_coordinates}
\end{figure}
In figure \ref{fig:image_coordinates}, $^nO$ denotes the origin of the normalized image plane, $^iO$ is the origin of the physical image plane, $^wO$ is the origin of the world frame, and $^cO$ is the origin of the camera 3D frame. Since $^cO$, $^c\hat{p}$ and $^cP$ in figure \ref{fig:image_coordinates} are collinear, we have that $\overrightarrow{^cO\hat{p}}=\lambda\overrightarrow{^cOP}$ for some constant $\lambda$ such that
\begin{equation}
\begin{cases}
    \hat{x}=\lambda X \\
    \hat{y}=\lambda Y \Longleftrightarrow \lambda = \frac{\hat{x}}{X}=\frac{\hat{y}}{Y}=\frac{1}{Z}\\
    1 = \lambda Z
\end{cases}
\end{equation}
which gives the following relation between the normalized plane coordinates $^n\hat{p}$ and the coordinates of the point $^cP$
\begin{equation}
\label{eq:perspectiveproj}
    \begin{cases}
    \hat{x}=\frac{X}{Z}\\
    \hat{y}=\frac{Y}{Z}
    \end{cases}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Homogeneous Coordinates and Rigid Transformations}
\label{section:transformations}
Homogeneous coordinates are convenient for representing geometric transformations by a matrix product.
Consider a point $P$ in some right-handed coordinate frame with origin $O$ and the $x$, $y$ and $z$ axis given by the unit vectors $\mathbf{i}$, $\mathbf{j}$ and $\mathbf{k}$ respectively. The point can be specified by
\begin{equation}
    \overrightarrow{OP}=X\mathbf{i}+Y\mathbf{j}+Z\mathbf{k}
\end{equation}
The nonhomogeneous coordinate vector $P$ is the vector $(X, Y, Z)^T$ in $\mathbb{R}^3$, while its homogeneous counterpart is the vector $(X,Y,Z,1)^T$ in $\mathbb{R}^4$. The change of coordinates between two (arbitrary) Euclidean coordinate systems $a$ and $b$ can be represented by a rotation matrix $\mathcal{R}_b^a$ and a translation vector $\mathbf{t}$ in $\mathbb{R}^3$ as
\begin{equation}
    \mathbf{P}^a=\mathcal{R}_b^a\mathbf{P}^b+\mathbf{t}
\end{equation}
Using homogeneous coordinates, the same transformation can be written as
\begin{equation}
\label{eq:homo_trans}
    \mathbf{P}^a=\mathcal{T}\mathbf{P}^b,\quad \text{where}\quad \mathcal{T}=\begin{bmatrix}\mathcal{R}_b^a & \mathbf{t} \\
    \mathbf{0}^T & 1 \end{bmatrix}
\end{equation}
where $\mathbf{P}^a$ and $\mathbf{P}^b$ are now vectors in $\mathbb{R}^4$. There are several ways to parameterize the rotation matrix $\mathcal{R}$, such as by Euler angles or quaternions. For details on parameterizations of rotation matrices, the reader is referred to \cite{modsim}, \cite{kinematics} and \cite{geometry}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Intrinsic Parameters}
\label{section:intrinsic_params}
Using homogeneous coordinates, equation \ref{eq:perspectiveproj} can be rewritten as
\begin{equation}
\label{eq:homo}
\mathbf{\hat{p}}^n=\frac{1}{Z}\begin{bmatrix}\mathbf{I}^{3\times3} & \mathbf{0}^{3\times1}\end{bmatrix}^c\mathbf{P}
\end{equation}
where $\mathbf{I}^{3\times3}$ is the 3-by-3 identity matrix, and $\mathbf{\hat{p}}^n\triangleq \begin{bmatrix}\hat{x}&\hat{y}&1\end{bmatrix}^T$ is the homogeneous coordinate vector of the projection $\hat{p}$ of the point $P$ into the normalized image plane, and $\mathbf{P}^c$ is the homogeneous coordinate vector of the point $P$ in the camera coordinate frame. The following section relates the homogeneous coordinates of the normalized image plane to those of the physical image plane. The physical retina of a camera is located at a distance $f\neq1$ from the pinhole (this assumes that the camera is focused at infinity, so that the physical distance from the pinhole to the image plane equals the focal length), and the physical image point $p$ is usually expressed in pixel units. In addition to this, pixels may be rectangular instead of square, introducing two additional scale parameters $k$ and $l$, such that
\begin{equation}
\label{eq:physical_plane_scaled}
    \begin{cases}
    x=kf\frac{X}{Z}=kf\hat{x}\\
    y=lf\frac{Y}{Z}=kf\hat{y}
    \end{cases}
\end{equation}
We have that $f$ represents a distance, in meters say, and a pixel has dimensions $\frac{1}{k}\times\frac{1}{l}$, where $k$ and $l$ are expressed in $\frac{\text{pixel}}{m}$. The parameters can be replaced by a magnification $\alpha=kf$ and $\beta=lf$ expressed in pixels. In general the origin of the physical image plane is in a corner $c$ of the physical image plane (often the upper left corner, but this may vary) and not at it's center, and the center of the CCD matrix typically does not coincide with the image center $c_0$. This adds two more parameters $x_0$ and $y_0$ that define the position of $c_0$ in pixel units in the physical image coordinate system. Equation \ref{eq:physical_plane_scaled} is then replaced by
\begin{equation}
\label{eq:physical_plane_scaled_and_translated}
    \begin{cases}
    x=\alpha\hat{x}+x_0\\
    y=\beta\hat{y}+y_0
    \end{cases}
\end{equation}
This assumes that the angle between the image axes equals exactly 90 degrees. In reality, however, the image plane axes may be skewed, such that the angle between the image plane axes is $\theta\neq90\degree$. In this case, equation \ref{eq:physical_plane_scaled_and_translated} is replaced by
\begin{equation}
\label{eq:physical_plane_scewed}
    \begin{cases}
    x=\alpha\hat{x}-\alpha\cot{\theta}\hat{y}+x_0\\
    y=\frac{\beta}{\sin{\theta}}\hat{y}+y_0
    \end{cases}
\end{equation}
In matrix form, equation \ref{eq:physical_plane_scewed} can be written as
\begin{equation}
\label{eq:calibr}
    \mathbf{p}=\mathcal{K}\mathbf{\hat{p}}, \quad \mathbf{p}=\begin{bmatrix}x\\y\\1\end{bmatrix},\quad \mathcal{K}\overset{\text{def}}{=}\begin{bmatrix}\alpha & -\alpha\cot{\theta} & x_0\\0 & \frac{\beta}{\sin{\theta}} & y_0\\0 & 0 & 1\end{bmatrix}
\end{equation}
The matrix $\mathcal{K}$ is called the \textit{calibration matrix} of the camera. Putting the equations \ref{eq:homo} and \ref{eq:calibr} together, we obtain
\begin{equation}
\label{eq:intrinsics}
    \mathbf{p}^i=\frac{1}{Z}\mathcal{K}\begin{bmatrix}\mathbf{I}^{3\times3} & \mathbf{0}^{3\times1}\end{bmatrix}\mathbf{P}^c=\frac{1}{Z}\mathcal{M}\mathbf{P}^c,\quad \text{where}\quad \mathcal{M}\overset{\text{def}}{=}\begin{bmatrix}\mathcal{K} & \mathbf{0}^{3\times1}\end{bmatrix}
\end{equation}
The parameters $\alpha$, $\beta$, $\theta$, $x_0$ and $y_0$ are called the \textit{intrinsic parameters} of the camera, typically obtained through a calibration procedure like the one described in section \ref{app:cam_calibration}.
\subsubsection{Extrinsic Parameters}
Equation \ref{eq:intrinsics} is written in a coordinate frame rigidly attached to the camera. However, for applications like autonomous vehicles, where the vehicle itself moves in some inertial coordinate system, we can express equation \ref{eq:intrinsics} in some world coordinate system using a rigid homogeneous transformation. The change of coordinates between the camera frame and the world frame can be expressed as presented in equation \ref{eq:homo_trans}:
\begin{equation}
    \mathbf{P}^c=\begin{bmatrix}\mathcal{R} & \mathbf{t}\\ \mathbf{0}^T & 1\end{bmatrix}\mathbf{P}^w
\end{equation}
Here, $\mathbf{P}^w$ represents the point $P$ in some world coordinate frame $w$. Substituting this into equation \ref{eq:intrinsics} yields
\begin{equation}
\label{eq:extrinsics}
    \mathbf{p}^i=\frac{1}{Z}\mathcal{M}{}^w\mathbf{P},\quad \text{where}\quad \mathcal{M}=\mathcal{K}\begin{bmatrix}\mathcal{R} & \mathbf{t}\end{bmatrix}
\end{equation}
Knowing $\mathcal{M}$ determines the position of the camera's optical center in the world coordinate frame $w$. A rotation matrix $\mathcal{R}$ is defined by three independent parameters (such as Euler angles), and together with the translation vector $\mathbf{t}$ defines six \textit{extrinsic parameters} that define the position and orientation of the camera relative to the world coordinate frame. The depth $Z$ in equation \ref{eq:extrinsics} is dependent on $\mathcal{M}$ and $\mathbf{P}^w$, which can be seen directly from the equation. If we denote the three rows of $\mathcal{M}$ as $\mathbf{m}_1^T$, $\mathbf{m}_2^t$ and $\mathbf{m}_3^T$, it follows that $Z=\mathbf{m}_1^T\cdot {}^w\mathbf{P}$. It's important to note that the matrix $\mathcal{M}$ is only defined up to scale, and multiplying $\mathcal{M}$ by a arbitrary constant $\lambda\neq0$ does not change the resulting image coordinates since
\begin{equation}
\begin{cases}
    {}^i x=\frac{\mathbf{m}_1^T\cdot {}^w\mathbf{P}}{\mathbf{m}_3^T\cdot {}^w\mathbf{P}}\\
    {}^i y=\frac{\mathbf{m}_2^T\cdot {}^w\mathbf{P}}{\mathbf{m}_3^T\cdot {}^w\mathbf{P}}
\end{cases}
\end{equation}
and the constant $\lambda$ cancels out. The perspective projection matrix $\mathcal{M}$ can be written explicitly as a function of its five intrinsic parameters, the rows $\mathbf{r}_1^T$, $\mathbf{r}_2^T$ and $\mathbf{r}_3^T$ of the rotation matrix $\mathcal{R}$ and the coordinates of of the vector $\mathbf{t}=\begin{bmatrix}t_1 & t_2 & t_3\end{bmatrix}^T$ as 
\begin{equation}
    \mathcal{M}=\begin{bmatrix}\alpha\mathbf{r_1}-\alpha\cot{\theta}\mathbf{r}_2^T+x_0\mathbf{r}_3^T & \alpha t_1-\alpha\cot{\theta}t_2+x_0t_3\\ \frac{\beta}{\sin{\theta}}\mathbf{r}_2^T + y_0\mathbf{r}_3^T & \frac{\beta}{\sin{\theta}}t_2+y_0t_3 \\
    \mathbf{r}_3^T & t_3\end{bmatrix}
\end{equation}
The intrinsic parameters are typically found through a calibration routine, while the extrinsic parameters depend on the location of the camera frame relative to the world frame the user chooses. A detailed explanation of camera calibration can be found in appendix \ref{app:cam_calibration}.
%\subsection{Cameras installed on ReVolt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LIDAR %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Lidar}
A lidar is an \textit{active} electro-optical sensor that sends out a laser pulse, and subsequently measures the parameters of the return signal bounced off some object. The lidar is known under several names, some examples being ladar, lidar, LIDAR, LADAR or laser radar. The term lidar is the most common, and will be used throughout this report. Lidars have seen a growing range of applications in recent years, with the current drive towards autonomous, driverless cars as a significant driving factor \todo{Kilde}. In a lidar, a waveform generator generates a laser waveform. Depending on the type of lidar, the setup can include a single laser or a master oscillator with multiple lasers or laser amplifiers. There are many types of lidars, however this report focuses on a particular type of 3D lidar with a rotating detector array, which measures azimuth and elevation angle, and range. 

The measurement is performed by the laser pulse being guided through transmit optics, traversing some medium, typically atmosphere, to a target. The laser pulse bounces off the target, and traverses the media again until receive optics captures the reflected pulse, guiding it to a detector or a detector array \cite{SpieLidar}. The laser and detector arrays rotate, taking multiple measurements as it scans the full 360 degree field of view. The range to a target can be determined based on the travel time of the laser pulse by
\begin{equation}
R=\frac{c}{2}(t_{rx}-t_{tx})
\end{equation}
where $c$ is the speed of light in the intervening medium, $t_{tx}$ and $t_{rx}$ is the transmission and reception time of the laser pulse, respectively. The laser travels twice the distance to the target between transmission and reception time, hence the resulting range is halved. The range resolution is determined by 
\begin{equation}
\Delta R=\frac{c}{2B}
\end{equation}
where $B$ is the system bandwidth, typically given by the transmit signal bandwidth.

\subsection{Velodyne VLP-16 Lidar Specifications}
The lidar used in this project is the Velodyne VLP-16, which is a small, real-time rotating lidar which steams data over a TCP/IP connection when it is powered up. The lidar has multiple return modes, where it can report either the strongest return signal, the last return signal, or both. The default return mode is to report the strongest return, and this mode is what is used in this project. The Velodyne VLP-16 features 16 laser/detector pairs mounted in a rotating housing, rapidly spinning to produce 360$\degree$ 3D image.\smallskip \\
Some features of the VLP-16 are \cite{velodyne_vlp16}:
\begin{itemize}
	\item Horizontal FOV of 360$\degree$.
	\item Weight of 830 grams.
	\item Rotational speed of 5-20 RPS (adjustable).
	\item Vertical FOV of 30$\degree$ (+15$\degree$ to -15$\degree$).
	\item Vertical angular resolution of 2\degree.
	\item Horizontal angular resolution of 0.1$\degree$-0.4$\degree$.
	\item Range of up to 100 meters (range depends on application).
	\item Accuracy $\pm3$ cm (typical).
	\item 903 nm wavelength laser.
\end{itemize}
The Velodyne VLP-16 is shown in figure \ref{fig:vlp_16}.
\begin{figure}[H]
	\centering
	\includegraphics[width=.4\linewidth]{fig/Velodyne_LiDAR_Puck_VLP-16_Left_900.png}
	\caption{The Velodyne VLP-16 lidar.}
	\label{fig:vlp_16}
\end{figure}
\subsection{Lidar Sensor Model}
\label{section:lidar_model}
A lidar is an optical detector, similar to a digital camera in that it responds to the intensity of the light hitting the detector, generating a voltage equal to the square of the intensity of the impinging light \cite{SpieLidar}. A rotating 3D lidar returns the intensity of the measured signal along with the range calculated from the travel time, as well as the azimuth (rotation) angle and the vertical angle of the point. The lidar has its own reference frame (in sperical coordinates), and it keeps track of its laser array angle, giving the azimuth angle of a point relative to some reference angle (typically determined by the manufacturer), and the vertical angle is referenced to the horizontal plane in the lidar frame of reference. The coordinates of a point in the lidar frame therefore consists of two angles, $\alpha$ and $\beta$, as well as the distance $R$ from the lidar to the point. Figure \ref{fig:lidar_spherical} illustrates this.
\begin{figure}[H]
    \centering
    \includegraphics[width=.4\linewidth]{fig/sperical_to_cartesian.png}
    \caption{Lidar spherical coordinate system, with equivalent cartesian coordinates.}
    \label{fig:lidar_spherical}
\end{figure}
In figure \ref{fig:lidar_spherical}, $\alpha$ is the azimuth angle, relative to the y-axis. $\beta$ is the vertical/elevation angle, and $R$ is the distance from the lidar to the point $P$.
From figure \ref{fig:lidar_spherical}, using basic trigonometry, we get that the transformation from spherical to cartesian coordinates is given by
\begin{equation}
\label{eq:spheric_to_cartesian}
    \begin{split}
        X&=P'\sin{\alpha}=R\cos{\beta}\sin{\alpha}\\
        Y&=P'\cos{\alpha}=R\cos{\beta}\cos{\alpha}\\
        Z&=R\sin{\beta}
    \end{split}
\end{equation}
The range returned by the Velodyne VLP-16 lidar is measured along a beam at known azimuth angles $\alpha$, and the 16 laser/detector pairs are mounted vertically at 2$\degree$ intervals from -15$\degree$ to 15$\degree$ referenced to the horizon. When the lidar is rotating at a frequency of 10 Hz the lasers fire at every $0.2\degree$, with a total of $1800$ laser firings for a full rotation. The theoretical maximum number of range measurements for a full $360\degree$ scan is therefore $360/0.2\times16=28800$ measurements, where it is assumed that every single laser beam is reflected, and subsequently detected at the lidar.
\begin{figure}[H]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.7\linewidth]{fig/lidar_side.png}
		\caption{Side view.}
		\label{fig:sub_lidarframe}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.7\linewidth]{fig/lidar_top.png}
		\caption{Top view.}
		\label{fig:sub_lidarframe2}
	\end{subfigure}
	\caption{The lidar cartesian coordinate frame, with angles from the spherical coordinate frame drawn in.}
	\label{fig:lidar_frames}
\end{figure}
A single reflection from the lidar can be represented in a cartesian coordinate frame centered in the lidar, shown in figure \ref{fig:lidar_frames}, using the transformation given in equation \ref{eq:spheric_to_cartesian} as:
\begin{equation}
\label{eq:simple_lidar}
\mathbf{P}^{l}_{ij}=\begin{bmatrix}
R_{i}\cos{\beta_i}\sin{\alpha}\\
R_{i}\cos{\beta_i}\cos{\alpha}\\
R_{i}\sin{\beta_i}
\end{bmatrix}
\end{equation}
Here, $\alpha$ is the azimuth angle in the scan, and $\beta_i$ is the angle of the $i^{\text{th}}$ vertical laser measured from the horizon. The superscript $l$ denotes the cartesian frame centered in the lidar. $R_{i}$ is the range measured at the given azimuth and elevation angle. This model assumes that the range measurement $R_{i}$ reported by the lidar is exact, as well as that the angles $\beta_i$ and $\alpha$ are known with perfect accuracy, which is generally not the case. Glennie et al. \cite{GlennieVLP} did a calibration and stability analysis of the VLP-16 lidar, where six calibration parameters for each laser was identified. The proposed model was given as
\begin{equation*}
\mathbf{P}^{l}_{ij}=\begin{bmatrix}
(s^iR_i+D_o^i)\cos{\beta_i}(\sin{\alpha}\cos{\delta_i}-\cos{\alpha}\sin{\delta_i})-H_o^i(\cos{\alpha}\cos{\delta_i}+\sin{\alpha}\sin{\delta_i})\\
(s^iR_i+D_o^i)\cos{\beta_i}(\cos{\alpha}\cos{\delta_i}+\sin{\alpha}\sin{\delta_i})-H_o^i(\sin{\alpha}\cos{\delta_i}-\cos{\alpha}\sin{\delta_i})\\
(s^iR_i+D_o^i)\sin{\beta_i}+V_o^i
\end{bmatrix}
\end{equation*}
where the calibration parameters are given in table \ref{tab:lidar_calib}.
\begin{table}[H]
	\centering
	\begin{tabularx}{.6\linewidth}{L L}
		\toprule
		$s^i$ & The distance scale factor for laser i\\
		\midrule
		$D_o^i$ & The distance offset for laser i \\
		\midrule
		$\beta_i$ & The vertical rotation correction for laser i \\
		\midrule
		$\delta_i$ & The horizontal rotation correction for laser i \\
		\midrule
		$V_o^i$ & the vertical offset from scanner frame origin for laser i \\
		\midrule
		$R_i$ & The raw distance measurement for laser i\\
		\midrule
		$\alpha$ & The encoder angle measurement\\
		\bottomrule
	\end{tabularx}
	\caption{Lidar calibration parameters.}
	\label{tab:lidar_calib}
\end{table}
Note that $\beta_i$ and $\alpha$ in table \ref{tab:lidar_calib} correspond to $\beta_i$ and $\alpha$ in equation \ref{eq:simple_lidar}. The Velodyne VLP-16 comes pre-calibrated from the factory, and Glennie et al. found that the factory supplied calibration is whithin the stated 3 cm ranging accuracy \cite{GlennieVLP}.
\subsection{Coordinate Transformation}
In order to use the measurements in conjunction with other measurements, such as those from a camera, they must be transformed to some common coordinate frame. As described in section \ref{section:transformations}, the measurement from the lidar can be transformed to another frame of reference by a rotation followed by a translation. If a navigation system gives the translation $\mathbf{t}^{w}_{wl}$ and the rotation $\mathcal{R}^{w}_{l}$ from the lidar frame to the world frame, the lidar measurement can be written in the world frame as
\begin{equation}
\mathbf{z}^w_{i} = \begin{bmatrix}
\mathbf{t}^w_{wl}+\mathcal{R}^{w}_{l}\mathbf{P}^{l}_{i}\\
\end{bmatrix}
\end{equation}
where $\mathbf{z}^w_{i}$ is a single point measurement given in the world frame.
\subsection{The Lidar Point Cloud}
For an entire 360$\degree$ scan, the measurement returned by the lidar can be represented as an aggregation of single point measurements as
\begin{equation}
\mathbf{Z}^w=\begin{bmatrix}
\mathbf{z}^{w}_{11} & \mathbf{z}^{w}_{12} & \dots & \mathbf{z}^{w}_{1j}\\
\mathbf{z}^{w}_{21} & \mathbf{z}^{w}_{22} & \dots & \mathbf{z}^{w}_{2j}\\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{z}^{w}_{i1} & \mathbf{z}^{w}_{i2} & \dots & \mathbf{z}^{w}_{ij}
\end{bmatrix}
\end{equation}
where, as before, $i$ denotes the index of the vertical laser (from 1 to 16 for the velodyne VLP-16), and $j$ is the index of the asimuth angle in the right-open interval $\interval[open right]{0\degree}{360\degree}$. When the Velodyne VLP-16 rotates at 10 Hz, $j$ ranges from 1 to 1800. $\mathbf{Z}^w$ represents a \textit{point cloud} made by a full rotation of the lidar, decomposed in the cartesian world frame.


\cleardoublepage