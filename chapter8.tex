%===================================== CHAP 5 =================================

\chapter{Conclusion and Future Work}
\section{Conclusion}
The results show that the proposed geometric calibration routine for the camera, as well as the coordinate transformation between the sensor frames, is valid. Detections of the target in images correlate well with detections in the lidar. Geometric calibration of the lidar was not deemed necessary, due to the accuracy of the factory calibration as shown by Glennie et al. \cite{GlennieVLP}.\\
\vspace{1mm}\\
\noindent Temporal calibration between both sensors was not achieved, mainly due to the difficulty in determining when a frame was captured in the camera. The camera used in the project is a commercial off-the-shelf action camera, which offers little support for the user to control the capturing of individual frames, or determining with a reasonable degree of accuracy when a individual frame was captured. This did not produce significant problems when the sensors were rigidly mounted, but in a sensor fusion application for a moving platform such as an ASV the sensors will need to be temporally calibrated. A different camera will be necessary to achieve this.\\ \vspace{1mm}\\
The lidar performed well in detecting the target boat, where no obviously false returns were reported. The water surface produced no returns, and the target boat was visible at all times inside a range of approximately 67 meters. The lidar returns known to originate from the background were removed manually in post-processing, so a methodology for distinguishing between uninteresting background returns and interesting objects will need to be developed. A potential method for eliminating returns from land could be to use land masking, as described by Wilthil et al. \cite{Wilthil2017}. This approach is dependent on the availability of accurate maps, as well as the position of the ASV being known through e.g. a GNSS system.
\\\vspace{1mm}\\
The analysis performed on the data generated show that it is possible to correlate detections in lidar data with detections in images, but the detections in the convolutional neural network is influenced by background objects, particularly when the target is far away, i.e. when it takes up little space in the image plane. Some of this interference could be due to the non-maxima suppression employed in Faster R-CNN, suppressing the detection of the target due to the potentially higher classification score of the background boat in the overlapping region proposals from the RPN \cite{ren15fasterrcnn}. The results show that the image detections are stable at close range, and when there is little clutter in the background confusing the detection of the target. The proposed model of Faster R-CNN could potentially be used as a close-range detector for smaller targets. Additional training of the network with a larger and more diverse custom data set could improve the performance, as could including more classes in the model. Given the stability of the detections from the lidar, and the relatively few false positives from the images, detections in images could be used to validate detected objects in the point cloud given by the lidar.\\\vspace{2mm}\\

\section{Future Work}
In order to continue the work towards sensor fusion, a different camera is needed. The camera used during this project is simply not suited for the task. A camera with a high-speed interface and the option to control the capture of individual frames would make it possible to temporally calibrate the camera with the lidar and the IMU. The calibration could be done using the framework of \textit{Kalibr} by Furgale et al. \cite{furgale_iros13}, which is a toolbox for spatial and temporal calibration of an IMU with respect to a camera system.\\\vspace{1mm}\\
A geometric calibration routine for the combined camera-lidar-IMU system should be performed, to get an accurate transformation between the sensor frames and the ASV body frame. This could be done for the lidar-camera transformation by using 3D-3D point correspondences as shown by Dhall et al. \cite{DBLP:journals/corr/DhallCRK17}, while the Kalibr-toolbox can give the transformation between the camera and the IMU \cite{furgale_iros13}. For the Faster R-CNN model, a larger custom data set should be produced, and background classes should be added to the model to eliminate false detections from the background.\\\vspace{1mm}\\
The end goal is to be able to detect and track targets in the vicinity of the ASV, based on the fused measurements from the lidar and the camera. In order to properly evaluate methodologies for fusion and tracking, more data should be gathered, with the sensors mounted on a test vehicle in a harbour environment to generate as realistic data as possible.\\\vspace{1mm}\\
A software architecture for real-time processing of the data must be developed in order to fuse the measurements from the lidar with detections from the camera on board the ASV. The ROS framework is ideal for such an application, and many of the neccessary software drivers and modules exist within the ROS community.
\cleardoublepage