%===================================== CHAP 1 =================================

\chapter{Introduction}

\section{Motivation}
Autonomous surface vehicles (ASVs) operating in urban environments, e.g. ferries, will need slightly different exteroceptive sensors than ASVs operating in the open sea. A lidar with a range of 100 meters may be more appropriate than a maritime radar with range of several kilometers. Furthermore, the complexity of the environment means that the rich information from optical cameras will be more important. In order to build a coherent world image, which, e.g., collision avoidance decisions can be based on, the data from these sensors must be fused, together with data from interoceptive sensor systems such as an inertial navigation system (INS). Sensor fusion is the process of combining observations from different sensor to provide a robust and complete description of an environment or process of interest \cite{Siciliano:2007:SHR:1209344}.
\section{Review}
Substantial research has been conducted into the field of remote sensing and data fusion with applications for autonomous vehicles, with a particular drive from the auto industry. Some examples of different approaches are given below.
\begin{itemize}
	\item Stiller et al. \cite{STILLER2000389} in 2000 proposed a multisensor concept with a variety of different sensor technologies with widely overlapping fields of view for an autonomous, unsupervised vehicle. They used stereo vision, laser scanners, radar, and short range radar, combined by sensor fusion into a joint obstacle map. The research was conducted as a part of the German project \textit{Autonomes Fahren}.
	
	\item Mälich et al. \cite{sensorfusion_spatio_temporal} used low-level fusion of multibeam lidar and vision sensor into a detection and tracking framework. They used a cascaded AdaBoost (adaptive boosting \cite{machine_learning}) detector based on haar-wavelet like features \cite{CValg}. Lidar returns were used to generate regions of interest in the images.
	
	\item Aufrère et al. \cite{Aufrere} at the NavLab group at Carnegie Mellon University proposed a high level fusion approach for object tracking using cameras and lidars for autonomous vehicles in cluttered urban environments. Their approach used a map-based fusion system, with a probability-based predictive model.
	
	\item Cho et al. \cite{Cho_multisensor_fusion} developed a multi-sensor system for moving object detection and tracking, building on the work of Aufrère et al. for the feature extraction for the lidar. Their approach fuses vision, lidar and radar. Detection in the vision module is represented as bounding boxes, and the data from the sensors were fused in an extended Kalman filter. The system detects and tracks pedestrians, bicyclists, and vehicles.
	
	\item Premebida et al. \cite{ROB:ROB20312} demonstrated a perception system for pedestrian detection in urban scenarios using information from lidar and a single camera. Two sensor fusion architectures are described in their paper. A  centralized architecture, where the fusion is done at the feature level, i.e. features from lidar and vision space combined in a single vector for posterior classification using a single classifier. The decentralized architecture employs two classifiers, one per sensor feature-space, fused by a trainable fusion method applied over the likelihoods provided by the component classifiers. They showed that the trainable fusion method lead to enhanced detection performance, and maintenance of false-alarms under tolerable values in comparison with single-based classifiers.
	
	\item Weigel et al. \cite{weigel_vehicle_tracking} demonstrated a vehicle tracking and lane detection multi-sensor system, using a lidar and a monocular camera. Detected vehicles are tracked and managed by a multi-object extended Kalman filter using the data from the lidar and the camera. The lidar was used to create appropriate regions of interest in the image plane, and subsequently the measurements in the image plane was incorporated into the Kalman filter.
\end{itemize}

There is also substantial research done for making autonomous vessels possible. Two examples which focus on the sensing and perception aspect of such systems are given below.
\begin{itemize}
	\item Wolf et al. \cite{ROB:ROB20371} describe the perception and planning systems of an autonomous surface vehicle with the goal to detect and track other vessels at medium to long ranges. They employ a NASA JPL developed tightly integrated system termed CARACaS (Control Architecture for Robotic Agent Command and Sensing) that blends the sensing, planning and behaviour autonomy necessary for such missions. In their paper, they presents an autonomy system that detects and tracks vessels of a defined class while patrolling near fixed assets. The sensor suite includes a wide-baseline stereo vision system for close-up perception and navigation, and a 360 degree camera head for longer range detection, identification, and tracking. The perception system termed SAVAnT (Surface Autonomous Visual Analysis and Tracking) receives sensory input from 6 cameras, stabilized by INS pose, detects objects of interest and calculates absolute bearings for each contact. The applications discussed in the paper is of a military nature.
	\item Elkins et al. \cite{ROB:ROB20367} published a paper on the Autonomous Maritime Navigation project, with the stated goal of creating a set of sensors, hardware, and software that enables autonomy on unmanned surface vehicle (USV) platforms. The sensor suite includes cameras, radar, lidar, compass and GPS, integrated into a sensor fusion engine. Fusion algorithms are used to compile and correlate these data into a common tactical picture for the USV. In their paper, they showed the benefits of using a lidar for close-range detections over more long-range sensors such as radars. They also showed that the wavelength of the lasers in the lidar is such that the radiated energy does not reflect well of the water surface, i.e. most points returned are not water, making it well suited in the detection of obstacles.
\end{itemize} 

\section{Report outline}
This report is divided into seven chapters. Following the introduction, background theory is presented in chapter 2. The geometric sensor models for the camera and the lidar, which is a prerequisite for sensor fusion, is presented in chapter 3. Chapter 4 presents some of the implementation details, the software and hardware used in the data collection and analysis, and the calibration procedure used for the camera. In chapter 5, the experiments performed in this project are presented, as well as the location used. In chapter 6, the results are presented and discussed. Chapter 7 presents the conclusion of the project, and suggests further work.

\cleardoublepage