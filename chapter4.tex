%===================================== CHAP 4 =================================

\chapter{Method}
\section{Sensors, Hardware and Processing Pipeline}
The physical sensor setup used in the data collection is illustrated in figure \ref{fig:sensor_setup}. A HP zbook 15 running Ubuntu 16.04 with ROS was used as a data collection platform.
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{fig/sensor_setup.png}
    \caption{Sensor setup with physical connections.}
    \label{fig:sensor_setup}
\end{figure}
The camera used to gather image data is a commercial off-the-shelf action camera by Muvi, named Veho K2 Sport. The reasoning for using such a camera was the affordable price, relatively high resolution, and a wide angle of view. A serious drawback with such a camera for computer vision applications, however, is the fact that it only sends images as a steady stream over its HDMI interface, with little options for the user to control the capture of individual frames. Moreover, HDMI is not a standard input interface on most computers, so a HDMI-to-USB3 adapter was purchased in order to be able to input the video stream on the computer.\\
\vspace{2mm}\\
\noindent The specifications of the computer used in the data gathering is listed in table \ref{tab:comp_specs_laptop}.
\begin{table}[H]
	\centering
	\begin{tabular}[H]{|c|c|}\hline
	Model & HP zbook 15\\\hline
	CPU & Intel Core i7-4800MQ 8$\times$2.7 GHz \\\hline
	Memory & 8 GB RAM \\\hline
	Graphics & lala \\\hline
	Storage & 250 GB Solid State HDD \\\hline
	Operating System & Linux Ubuntu 16.04 LTS (Xenial)\\
	\hline
	\end{tabular}
	\caption{Computer specifications.}
	\label{tab:comp_specs_laptop}
\end{table}
Overview of the sensor setup, hardware used, and signal flow through the system.
\section{Faster R-CNN}
Introduce the framework of Faster R-CNN
\subsection{Training and validation data}
Describe the datasets used in training and validation (Espen Tangstads work basically)
\subsection{Implementation Aspects}
Steps needed to run the Matlab implementation from https://github.com/ShaoqingRen/faster\_rcnn. Compiling Caffe with cuda for GTX1070, the link between caffe and Matlab (mex).
\section{ROS Implementation}
In order to capture, organize, timestamp and save the collected data, the open-source robotics framework of ROS was used. ROS, which is an abbreviation for Robot Operating System, is not, as the name might imply, a operating system, but a framework running on top of a traditional OS. The framework of ROS is based on several philosophical aspects \cite{programming_ros}:
\begin{itemize}
    \item \textit{Peer to peer}: A ROS system consists of several small programs (called nodes) connected to each other, continuously exchanging messages. Messages travel directly from one node to another.
    \item \textit{Tools-based}: Complex ROS systems can be created from many small, generic programs. ROS does not have a integrated development and runtime environment, and tasks such as (but not limited to) navigating the source code tree, visualizing the system interconnections, generating documentation, and logging data are performed by separate programs.
    \item \textit{Multilingual}: ROS software modules can be written in any language for which a \textit{client library} has been written. Client libraries exist for C++, Python, LISP, Java, JavaScript, MATLAB, Ruby, Haskell, R, and others. This provides flexibility for the programmer, in that he or she can choose the language they are most familiar with, or is best suited for the task, when creating new functionality. ROS achieves this multilinguality by enforcing a convention for serializing messages being passed between nodes.
    \item \textit{Thin}: ROS conventions encourage developers to create standalone libraries and then wrap those libraries so that they can send and receive messages to and from other ROS modules. This allows reuse of software outside ROS, and simplifies automated testing using continuous integration tools.
    \item \textit{Free and open source}: The core of ROS is released under the BSD license \cite{BSD}, which allows commercial and noncommercial use.
\end{itemize}
In ROS, \lstinline[basicstyle=\ttfamily]{roscore} is a service that provides connection information to nodes so that they can transmit messages to and from one another. Each node connects to \lstinline[basicstyle=\ttfamily]{roscore} on startup to register details of the messages it publishes, and the details of the messages to which it wishes to subscribe. \lstinline[basicstyle=\ttfamily]{roscore} is not a server in the traditional client/server sense, all messages are sent peer-to-peer between nodes. The \lstinline[basicstyle=\ttfamily]{roscore} service is only used by nodes to know where to find their peers. Due to this fact, every ROS system needs a running \lstinline[basicstyle=\ttfamily]{roscore}. Upon startup, every ROS node expects its process to have an environment variable \lstinline[basicstyle=\ttfamily]{ROS_MASTER_URI}, containing a string in the format \lstinline[basicstyle=\ttfamily]{http://hostname:portnumber}, to tell it where the \lstinline[basicstyle=\ttfamily]{roscore} service is running, and which port it is accessible on. The connection between nodes, and between individual nodes and \lstinline[basicstyle=\ttfamily]{roscore} is illustrated in figure \ref{fig:roscore}.
\begin{figure}[H]
    \centering
    \includegraphics[width=.4\linewidth]{fig/roscore.png}
    \caption{Roscore and its connection to other nodes in the system.}
    \label{fig:roscore}
\end{figure}
The ephemeral connection between nodes and \lstinline[basicstyle=\ttfamily]{roscore} is illustrated by dashed lines, indicating periodic calls to \lstinline[basicstyle=\ttfamily]{roscore} to find peers, while the peer-to-peer message passing is illustrated by the solid line between the nodes.
The main ROS client libraries are geared towards a UNIX/Linux platform, mainly due to the dependency on large collections of open-source software dependencies \todo{kilde}. Ubuntu Linux is a supported distribution, while others, such as Fedora Linux, Mac OS, and Windows, are designated as experimental and are supported by the community. Due to the simplicity of installation and "out-of-the-box" functionality of ROS on Ubuntu Linux, the most recent long-term supported version of ROS, ROS Kinetic, was installed on a laptop running Ubuntu 16.04 LTS, which was used as a platform for running the various sensor drivers and collecting the data.
\subsection{ROS Nodes and Data Flow}
An overview of the implemented sensor driver and data processing nodes is illustrated in figure \ref{fig:ros_nodes}.
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{fig/ros.png}
    \caption{ROS nodes and topic flow.}
    \label{fig:ros_nodes}
\end{figure}
The lidar nodes, camera node and IMU node are all based on open-source ROS packages freely available from the ROS community. The camera driver used is the \\\lstinline[basicstyle=\ttfamily]{libuvc_camera} driver. This node reads the image stream from the camera over the USB port, providing a ROS interface for cameras meeting the USB Video Class standard \cite{libuvc_camera}. This node publishes messages containing an image corresponding to a single frame of the video stream, with a time stamp corresponding to the arrival time of the image frame over USB. The source code is available at the ROS device drivers repository \cite{ros_drivers}.\\
\vspace{2mm}\\
\noindent The lidar nodes are in the \lstinline[basicstyle=\ttfamily]{velodyne_driver}-package from the ROS device drivers repository \cite{ros_drivers}. This package provides two nodes, the first is \lstinline[basicstyle=\ttfamily]{/velodyne_driver} for collecting all TCP packages sendt over ethernet from the lidar and publishing a message containing the raw data from a full 360-degree scan from the lidar. The second node, \lstinline[basicstyle=\ttfamily]{/velodyne_cloud}, transforms the raw data into a point cloud given in the frame of reference the user chooses (default \lstinline[basicstyle=\ttfamily]{/velodyne}, the sensor frame) in XYZ-coordinates. In addition to the coordinates of each point, the corresponding intensity of the reflected signal strength and the ring data is published \cite{velodyne_driver}.\\ 
\vspace{2mm}\\
\noindent The point cloud is then published as a ROS message. In addition to the coordinates of each point, the corresponding intensity of the reflected signal strength and the ring data is also included in this message \cite{velodyne_driver}.\\
\vspace{2mm}\\
\noindent The driver for the IMU is the \lstinline[basicstyle=\ttfamily]{xsens_driver} package \cite{xsens_driver}, which communicates with the Xsens IMU over USB, and publishes messages containing the measurements from the IMU and GNSS position measurements \cite{xsens_driver}.\\
\vspace{2mm}\\
\noindent The dummy AIS nodes in figure \ref{fig:ros_nodes} are nodes for receiving and processing GNSS-information received over the internet from a mobile phone, developed at NTNU by the Autosea-project \todo{siter autosea et eller annet}. These nodes are used for receiving and storing the position ground truth for the vessel used in the experiments along with all the other data. They provide raw data as an NMEA-0183 sentence, which is a standard defined by the National Maritime Electronics Association for communication between marine electronics, as well as speed over ground and GNSS position data.
The details of messages passed between nodes in the system is given in table \ref{tab:topics}.
\begin{table}[H]
	\centering
	\begin{tabularx}{\linewidth}{|c|L|}\hline
		\textbf{Topic} & \textbf{Content} \\\hline
		\lstinline[basicstyle=\ttfamily]{nmea\_msgs/Sentence} & GNSS Data as a String representing a NMEA0183 sentence \\\hline
		\lstinline[basicstyle=\ttfamily]{sensor\_msgs/NavSatFix} & Navigation Satellite fix specified using WGS 84 ellipsoid\\\hline
		\lstinline[basicstyle=\ttfamily]{geometry\_msgs/TwistStamped} & Linear and angular velocity\\\hline
		\lstinline[basicstyle=\ttfamily]{autosea\_msgs/AisDynamic} & AIS message generated from GNSS data\\\hline
		\lstinline[basicstyle=\ttfamily]{sensor\_msgs/TimeReference} & External time reference not actively synchronized with system time (i.e. GNSS time) \\\hline
		\lstinline[basicstyle=\ttfamily]{sensor\_msgs/PointCloud2} & Message containing point cloud data from the lidar \\\hline
		\lstinline[basicstyle=\ttfamily]{velodyne\_msgs/VelodyneScan} & Message containing raw data from a 360$\deg$ lidar scan \\\hline
		\lstinline[basicstyle=\ttfamily]{sensor\_msgs/CompressedImage} & Message containing a compressed image in .JPEG format\\\hline
		\lstinline[basicstyle=\ttfamily]{sensor\_msgs/Imu} & Message containing raw measurements from the IMU\\\hline
		\end{tabularx}
	\caption{ROS Topics used in data collection.}
	\label{tab:topics}
\end{table}
\subsection{Rosbag}
ROS provides a built-in set of tools for recording and playing back ROS topics, \lstinline[basicstyle=\ttfamily]{rosbag} \cite{rosbag}. This toolset stores the topics of interest, specified as parameters in the command-line, to a file of the ROS bag format, with extension .bag. This toolset provides a simple method for storing data from experiments, and the playback function provides the option to simulate sensor inputs to the system without actually reading sensors. A basic use case, where for example two topics named \lstinline[basicstyle=\ttfamily]{/imu_data} and \lstinline[basicstyle=\ttfamily]{/camera/image} are to be recorded, the proper command line input is 
\begin{lstlisting}[basicstyle=\ttfamily]
	$ rosbag record -o filename /imu_data /camera/image
\end{lstlisting}
where \lstinline[basicstyle=\ttfamily]{filename} is the user-specified filename for the .bag file. ROS appends the filename with the system time automatically. Rosbag was used in recording all experimental data.
\subsection{MATLAB implementation}
MATLAB provides ROS interfaces via the Robotics System Toolbox, making it possible to use ROS topics directly in MATLAB. 
\section{Camera Calibration}
\label{section:matlab_calibration}
As explained in section \ref{section:intrinsic_params}, one needs the cameras intrinsic parameters in order to associate pixel coordinates with world coordinates. MATLABs Computer Vision System Toolbox includes the Camera Calibrator application, which was used in calibrating the camera. The Camera Calibrator application implements the calibration routine by Zhang \cite{flexCal}, described in appendix \ref{app:cam_calibration} \cite{camera_calibrator}. The steps taken for calibrating the camera using this application can be summarized as follows:
\begin{enumerate}
	\item Prepare a checkerboard calibration pattern with known dimensions.
	\item Capture 20-40 images of the checkerboard with the camera, from varying orientations.
	\item Initiate the calibration.
	\item Evaluate calibration accuracy.
	\item Adjust parameters to improve accuracy (if necessary).
	\item Export parameters as a cameraParameters object.
\end{enumerate}
The accuracy in the calibration routine can be improved by removing outlier images, which contribute more to the reprojection errors than others. Figure \ref{fig:calib_outlier} shows an example of a blurred image, which due to the error introduced to keypoint detection by the blurryness of the image causes a larger reprojection error.
\begin{figure}[H]
	\centering
	\includegraphics[width=.8\linewidth]{outlier_calibration.PNG}
	\caption{Camera Calibration app showing an outlier image.}
	\label{fig:calib_outlier}
\end{figure}
The outliers can be removed by adjusting the threshold for mean pixel reprojection error shown in the upper right corner of figure \ref{fig:calib_outlier}. The calibration app also computes the extrinsic parameters and displays the calibration pattern planes in 3D, as seen in the lower right corner in figure \ref{fig:calib_outlier}.
Having calibrated the camera, the radial distortion in the images can be removed, as seen in figure \ref{fig:distortion}, where a unprocessed image with the calibration pattern key points overlaid can be seen next to the undistorted version.
\begin{figure}[H]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.8\linewidth]{fig/calibration_distorted.PNG}
		\caption{Original image with radial distortion.}
		\label{fig:sub_distort1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.8\linewidth]{fig/calibration_undistorted.PNG}
		\caption{Undistorted image.}
		\label{fig:sub_distort2}
	\end{subfigure}
	\caption{Distorted checkerboard image, along with the undistorted version.}
	\label{fig:distortion}
\end{figure}
The numerical values for the intrinsic matrix was found using this routine as
\begin{equation}
\mathcal{K}=\begin{bmatrix}\alpha & -\alpha\cot{\theta} & x_0\\0 & \frac{\beta}{\sin{\theta}} & y_0\\0 & 0 & 1\end{bmatrix}=\begin{bmatrix}
1068.7558 & -1.9413 & 893.7339 \\ 0 & 1066.6539 &516.2405\\ 0 & 0 & 1
\end{bmatrix}
\end{equation}  
\subsection{Time Synchronization}
As mentioned in section \ref{section:fusion}, the measurements from the camera need to be time synchronized with the other measurements, in order to associate a image frame with the data from the lidar and the IMU. blablabla....
\cleardoublepage